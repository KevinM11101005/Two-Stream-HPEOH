{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEX_YCB\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import copy\n",
    "import torchvision.transforms as transforms\n",
    "from common.logger import colorlogger\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from common.utils.preprocessing import load_img, get_bbox, process_bbox, generate_patch_image, augmentation\n",
    "from main.config import cfg\n",
    "from common.utils.transforms import compute_mpjpe, compute_pa_mpjpe\n",
    "from common.utils.skeleton_map import skeleton_map_gray\n",
    "from common.codecs.keypoint_eval import keypoint_pck_accuracy\n",
    "from mmpose.utils.tensor_utils import to_numpy\n",
    "from common.utils.vis import ux_hon_result, ux_hon_result_final\n",
    "\n",
    "class DEX_YCB(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, data_split, log_name='cfg_logs.txt'):\n",
    "        self.transform = transform\n",
    "        self.data_split = data_split if data_split == 'train' else 'test'\n",
    "        self.root_dir = osp.join('data', 'DEX_YCB')\n",
    "        self.annot_path = osp.join(self.root_dir, 'annotations')\n",
    "        self.hand_type = {'left': 0, 'right': 0}\n",
    "        self.datalist = self.load_data()\n",
    "        self.root_joint_idx = 0\n",
    "        if self.data_split != 'train':\n",
    "            self.eval_result = [[],[],[],[]] #[mpjpe_list, pa-mpjpe_list]\n",
    "        \n",
    "        self.logger = colorlogger(cfg.log_dir, log_name=log_name)\n",
    "        \n",
    "        if self.data_split == 'train':\n",
    "            for i in cfg.__dict__:\n",
    "                self.logger.info('{0}: {1}'.format(i, cfg.__dict__[i]))\n",
    "        \n",
    "        message = []\n",
    "        message.append(f\"DataList len: {len(self.datalist)}\")\n",
    "        message.append('left hand data: {0}, right hand data: {1}'.format(self.hand_type['left'], self.hand_type['right']))\n",
    "        \n",
    "        if cfg.simcc and cfg.SET:\n",
    "            message.append(f'Start the model {cfg.backbone} with SET and with simcc')\n",
    "        elif cfg.simcc:\n",
    "            message.append(f'Start the model {cfg.backbone} without SET and with simcc')\n",
    "        elif cfg.SET:\n",
    "            message.append(f'Start the model {cfg.backbone} with SET and with regressor')\n",
    "        else:\n",
    "            message.append(f'Start the model {cfg.backbone} without SET and with regressor')\n",
    "        for msg in message:\n",
    "            self.logger.info(msg)\n",
    "            \n",
    "    def load_data(self):\n",
    "        db = COCO(osp.join(self.annot_path, \"DEX_YCB_s0_{}_data.json\".format(self.data_split)))\n",
    "        \n",
    "        datalist = []\n",
    "        skip = 1\n",
    "\n",
    "        if self.data_split == 'train':\n",
    "            skip_mode = cfg.train_skip\n",
    "            remainder = cfg.train_remainder\n",
    "        elif self.data_split == 'test':\n",
    "            skip_mode = cfg.test_skip\n",
    "            remainder = cfg.test_remainder\n",
    "\n",
    "        for aid in db.anns.keys():\n",
    "            if skip % skip_mode == remainder:\n",
    "                ann = db.anns[aid]\n",
    "                image_id = ann['image_id']\n",
    "                img = db.loadImgs(image_id)[0]\n",
    "                if osp.exists(osp.join(self.root_dir, img['file_name'])):\n",
    "                    img_path = osp.join(self.root_dir, img['file_name'])\n",
    "                    img_shape = (img['height'], img['width'])\n",
    "                    \n",
    "                    joints_coord_img = np.array(ann['joints_img'], dtype=np.float32)\n",
    "                    hand_type = ann['hand_type']\n",
    "\n",
    "                    bbox = get_bbox(joints_coord_img[:,:2], np.ones_like(joints_coord_img[:,0]), expansion_factor=1.5)\n",
    "                    bbox = process_bbox(bbox, img['width'], img['height'], expansion_factor=1.0)\n",
    "\n",
    "                    data = {\"img_path\": img_path, \"img_shape\": img_shape, \"joints_coord_img\": joints_coord_img,\n",
    "                            \"bbox\": bbox, \"hand_type\": hand_type}\n",
    "                    \n",
    "                    if all(val is not None for val in data.values()):\n",
    "                        datalist.append(data)\n",
    "                        if data['hand_type'] == 'left':\n",
    "                            self.hand_type['left'] += 1\n",
    "                        else:\n",
    "                            self.hand_type['right'] += 1\n",
    "            skip += 1\n",
    "        return datalist\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = copy.deepcopy(self.datalist[idx])\n",
    "        img_path, img_shape, bbox = data['img_path'], data['img_shape'], data['bbox']\n",
    "        hand_type = data['hand_type']\n",
    "        do_flip = False # (hand_type == 'left')\n",
    "\n",
    "        # img\n",
    "        img = load_img(img_path)\n",
    "        orig_img = copy.deepcopy(img)[:,:,::-1]\n",
    "        img, img2bb_trans, bb2img_trans, rot, scale = augmentation(img, bbox, self.data_split, do_flip=do_flip)\n",
    "        # Convert numpy array to PIL Image\n",
    "        # img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        # img = Image.fromarray(img)\n",
    "        save_path = cfg.vis_dir + '/' + 'image'\n",
    "        save_path = save_path + '/' + str(idx) + '.jpg'\n",
    "        img = self.transform(img.astype(np.float32))/255.\n",
    "\n",
    "        if self.data_split == 'train':\n",
    "            targets = {}\n",
    "            ## 2D joint coordinate\n",
    "            joints_img = data['joints_coord_img']\n",
    "            # if do_flip:\n",
    "            #     joints_img[:,0] = img_shape[1] - joints_img[:,0] - 1\n",
    "            joints_img_xy1 = np.concatenate((joints_img[:,:2], np.ones_like(joints_img[:,:1])),1)\n",
    "            joints_img = np.dot(img2bb_trans, joints_img_xy1.transpose(1,0)).transpose(1,0)[:,:2]\n",
    "            if not cfg.simcc:\n",
    "                joints_img_copy = joints_img.copy()\n",
    "                ## normalize to [0,1]\n",
    "                joints_img_copy[:,0] /= cfg.input_img_shape[0]\n",
    "                joints_img_copy[:,1] /= cfg.input_img_shape[1]\n",
    "                targets['joints_img'] = joints_img_copy\n",
    "            else:\n",
    "                targets['joints_img'] = joints_img\n",
    "            \n",
    "            skeleton_map = skeleton_map_gray((cfg.input_img_shape[0], cfg.input_img_shape[1]), joints_img)\n",
    "            # cv2.imshow('test', skeleton_map)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "            skeleton_map = self.transform(skeleton_map.astype(np.float32))/255.\n",
    "\n",
    "            inputs = {'img': img}\n",
    "            targets['skeleton_map'] = skeleton_map\n",
    "        else:\n",
    "            inputs = {'img': img}\n",
    "            targets = {}\n",
    "\n",
    "        return inputs, targets\n",
    "    \n",
    "    def evaluate(self, outs, cur_sample_idx):\n",
    "        annots = self.datalist\n",
    "        sample_num = len(outs)\n",
    "        for n in range(sample_num):            \n",
    "            annot = annots[cur_sample_idx + n]\n",
    "            # cv2.namedWindow(annot['img_path'], 0)\n",
    "            \n",
    "            out = outs[n]\n",
    "            \n",
    "            # img convert\n",
    "            img = load_img(annot['img_path'])\n",
    "            orig_img = copy.deepcopy(img)\n",
    "            img, img2bb_trans, bb2img_trans, rot, scale = augmentation(img, annot['bbox'], self.data_split, do_flip=False)\n",
    "        \n",
    "    #         # GT and out['keypoints]\n",
    "            gt_joints_coord_img = annot['joints_coord_img']\n",
    "            joints_img_xy1 = np.concatenate((gt_joints_coord_img[:,:2], np.ones_like(gt_joints_coord_img[:,:1])),1)\n",
    "            joints_img = np.dot(img2bb_trans, joints_img_xy1.transpose(1,0)).transpose(1,0)[:,:2]\n",
    "            \n",
    "            if cfg.backbone == 'unext':\n",
    "                gt_skeleton_map = skeleton_map_gray((cfg.input_img_shape[0], cfg.input_img_shape[1]), joints_img)\n",
    "                gt_skeleton_map = gt_skeleton_map/255.\n",
    "                \n",
    "                pred_skeleton_map = (out['skeleton_map'].squeeze().cpu().numpy()).astype(float)# > 0.5\n",
    "                \n",
    "                ## show result\n",
    "                cat_imgs = ux_hon_result(orig_img, img, pred_skeleton_map, gt_skeleton_map)\n",
    "                cat_imgs = ux_hon_result_final(out, bb2img_trans, orig_img, img, cat_imgs)\n",
    "                \n",
    "                parts = re.split(r'[\\\\/]', annot['img_path'])\n",
    "                path = osp.join(cfg.vis_dir,'_'.join(parts[1:]))\n",
    "                cv2.imwrite(path, cat_imgs)\n",
    "                # cv2.imshow(annot['img_path'], cat_imgs)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "\n",
    "                pred_skeleton_map = (out['skeleton_map'].squeeze().cpu().numpy()>0.5).astype(float)# > 0.5\n",
    " \n",
    "                num_correct = (pred_skeleton_map==gt_skeleton_map).sum()\n",
    "                num_pixels = cfg.input_img_shape[0] * cfg.input_img_shape[1]\n",
    "            else:\n",
    "                img_uint8 = cv2.resize(orig_img.astype(np.uint8), (cfg.input_img_shape[0], cfg.input_img_shape[1]))\n",
    "                rgb_img_uint8 = cv2.cvtColor(img_uint8.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                rgb_img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                ori_imgs = np.hstack([rgb_img_uint8, rgb_img])\n",
    "                cat_imgs = ux_hon_result_final(out, bb2img_trans, orig_img, img, ori_imgs)\n",
    "                \n",
    "                parts = re.split(r'[\\\\/]', annot['img_path'])\n",
    "                path = osp.join(cfg.vis_dir,'_'.join(parts[1:]))\n",
    "                cv2.imwrite(path, cat_imgs)\n",
    "                # cv2.imshow(annot['img_path'], cat_imgs)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "                \n",
    "\n",
    "            if cfg.simcc:\n",
    "                pred_keypoints = np.expand_dims(out['keypoints'], axis=0)\n",
    "                pred_keypoint_scores = out['keypoint_scores']\n",
    "                keypoints_restored = np.dot(bb2img_trans, np.concatenate((pred_keypoints[0], np.ones((pred_keypoints[0].shape[0], 1))), axis=1).transpose(1, 0))\n",
    "                keypoints_restored = keypoints_restored[:2, :].transpose(1, 0)\n",
    "            else:\n",
    "                pred_keypoints = np.expand_dims(out['joints_coord_img'].cpu().numpy(), axis=0)\n",
    "                pred_keypoints[:,:,0] *= cfg.input_img_shape[1]\n",
    "                pred_keypoints[:,:,1] *= cfg.input_img_shape[0]\n",
    "                keypoints_restored = np.dot(bb2img_trans, np.concatenate((pred_keypoints[0], np.ones((pred_keypoints[0].shape[0], 1))), axis=1).transpose(1, 0))\n",
    "                keypoints_restored = keypoints_restored[:2, :].transpose(1, 0)\n",
    "                pred_keypoint_scores = np.any(keypoints_restored, axis=1)\n",
    "\n",
    "    #         # flip back to left hand\n",
    "    #         if annot['hand_type'] == 'left':\n",
    "    #             joints_out[:,0] *= -1 \n",
    "            _, avg_acc, _ = keypoint_pck_accuracy(\n",
    "                pred=np.expand_dims(keypoints_restored, axis=0),\n",
    "                gt=np.expand_dims(gt_joints_coord_img[:,:2], axis=0),\n",
    "                mask=np.expand_dims(pred_keypoint_scores, axis=0) > 0,\n",
    "                thr=cfg.pck_thr,\n",
    "                norm_factor=np.expand_dims(annot['img_shape'], axis=0),\n",
    "            )\n",
    "\n",
    "            self.eval_result[2].append(compute_mpjpe(keypoints_restored, gt_joints_coord_img[:,:2]))\n",
    "\n",
    "            if cfg.backbone == 'unext':\n",
    "                self.eval_result[0].append(num_correct / num_pixels)\n",
    "                self.eval_result[1].append(avg_acc)\n",
    "            else:\n",
    "                self.eval_result[0].append(avg_acc)\n",
    "                \n",
    "    def print_eval_result(self, test_epoch):\n",
    "        message = []\n",
    "        if cfg.backbone == 'unext':\n",
    "            message.append('Output: {0}, Model: snapshot_{1}.pth.tar'.format(cfg.output_dir.split('\\\\')[-1], test_epoch))\n",
    "            message.append('Correct/Total(One Batch) pixels: {0:.2f}'.format(np.mean(self.eval_result[0]) * 100))\n",
    "            message.append('PCK@{0}: {1:.2f}'.format(cfg.pck_thr, np.mean(self.eval_result[1]) * 100))\n",
    "            message.append('MPJPE : %.2f' % np.mean(self.eval_result[2]))\n",
    "        else:\n",
    "            message.append('Output: {0}, Model: snapshot_{1}.pth.tar'.format(cfg.output_dir.split('\\\\')[-1], test_epoch))\n",
    "            message.append('PCK@{0}: {1:.2f}'.format(cfg.pck_thr, np.mean(self.eval_result[0]) * 100))\n",
    "            message.append('MPJPE : %.2f' % np.mean(self.eval_result[2]))\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HO3D \n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "from common.logger import colorlogger\n",
    "from common.utils.skeleton_map import skeleton_map_gray\n",
    "from pycocotools.coco import COCO\n",
    "from main.config import cfg\n",
    "from common.utils.preprocessing import load_img, get_bbox, process_bbox, generate_patch_image, augmentation\n",
    "from common.utils.transforms import world2cam, cam2pixel, compute_mpjpe, compute_pa_mpjpe\n",
    "from common.utils.vis import vis_keypoints, vis_mesh, save_obj, vis_keypoints_with_skeleton\n",
    "from common.codecs.keypoint_eval import keypoint_pck_accuracy\n",
    "\n",
    "class HO3D(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, data_split, log_name='cfg_logs.txt'):\n",
    "        self.transform = transform\n",
    "        self.data_split = data_split if data_split == 'evaluation' else 'train'\n",
    "        self.root_dir = osp.join('data', 'HO3D')\n",
    "        self.annot_path = osp.join(self.root_dir, 'annotations')\n",
    "        self.root_joint_idx = 0\n",
    "        self.hand_type = {'left': 0, 'right': 0}\n",
    "        self.datalist = self.load_data()\n",
    "        if self.data_split == 'train':\n",
    "            self.eval_result = [[],[],[],[]]\n",
    "        \n",
    "        self.logger = colorlogger(cfg.log_dir, log_name=log_name)\n",
    "        \n",
    "        for i in cfg.__dict__:\n",
    "            self.logger.info('{0}: {1}'.format(i, cfg.__dict__[i]))\n",
    "        message = []\n",
    "        message.append(f\"DataList len: {len(self.datalist)}\")\n",
    "        message.append('left hand data: {0}, right hand data: {1}'.format(self.hand_type['left'], self.hand_type['right']))\n",
    "        \n",
    "        if cfg.simcc and cfg.SET:\n",
    "            message.append(f'Start the model {cfg.backbone} with SET and simcc')\n",
    "        elif cfg.simcc:\n",
    "            message.append(f'Start the model {cfg.backbone} without SET and with simcc')\n",
    "        elif cfg.SET:\n",
    "            message.append(f'Start the model {cfg.backbone} with SET and with regressor')\n",
    "        else:\n",
    "            message.append(f'Start the model {cfg.backbone} without SET and simcc')\n",
    "        for msg in message:\n",
    "            self.logger.info(msg)\n",
    "            \n",
    "    def load_data(self):\n",
    "        db = COCO(osp.join(self.annot_path, \"HO3D_{}_data.json\".format(self.data_split)))\n",
    "        # db = COCO(osp.join(self.annot_path, 'HO3Dv3_partial_test_multiseq_coco.json'))\n",
    "\n",
    "        datalist = []\n",
    "        skip = 1\n",
    "        if self.data_split == 'train':\n",
    "            skip_mode = cfg.train_skip\n",
    "            remainder = cfg.train_remainder\n",
    "        elif self.data_split == 'test':\n",
    "            skip_mode = cfg.test_skip\n",
    "            remainder = cfg.test_remainder\n",
    "\n",
    "        for aid in db.anns.keys():\n",
    "            if skip % skip_mode == remainder:\n",
    "                ann = db.anns[aid]\n",
    "                image_id = ann['image_id']\n",
    "                img = db.loadImgs(image_id)[0]\n",
    "                if osp.exists(osp.join(self.root_dir, self.data_split, img['file_name'])):\n",
    "                    img_path = osp.join(self.root_dir, self.data_split, img['file_name'])\n",
    "                    # TEMP\n",
    "                    # img_path = osp.join(self.root_dir, 'train', img['sequence_name'], 'rgb', img['file_name'])\n",
    "\n",
    "                    img_shape = (img['height'], img['width'])\n",
    "                    joints_coord_cam = np.array(ann['joints_coord_cam'], dtype=np.float32) # meter\n",
    "                    cam_param = {k:np.array(v, dtype=np.float32) for k,v in ann['cam_param'].items()}\n",
    "                    joints_coord_img = cam2pixel(joints_coord_cam, cam_param['focal'], cam_param['princpt'])\n",
    "                    bbox = get_bbox(joints_coord_img[:,:2], np.ones_like(joints_coord_img[:,0]), expansion_factor=1.5)\n",
    "                    bbox = process_bbox(bbox, img['width'], img['height'], expansion_factor=1.0)\n",
    "                    data = {\"img_path\": img_path, \"img_shape\": img_shape, \"joints_coord_img\": joints_coord_img,\n",
    "                            \"bbox\": bbox,}\n",
    "                        \n",
    "                    if all(val is not None for val in data.values()):\n",
    "                        datalist.append(data)\n",
    "                        self.hand_type['right'] += 1\n",
    "            skip += 1\n",
    "        return datalist\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = copy.deepcopy(self.datalist[idx])\n",
    "        img_path, img_shape, bbox = data['img_path'], data['img_shape'], data['bbox']\n",
    "\n",
    "        # img\n",
    "        img = load_img(img_path)\n",
    "        img, img2bb_trans, self.bb2img_trans, rot, scale = augmentation(img, bbox, self.data_split, do_flip=False)\n",
    "        img = self.transform(img.astype(np.float32))/255.\n",
    "\n",
    "        if self.data_split == 'train':\n",
    "            targets = {}\n",
    "            ## 2D joint coordinate\n",
    "            joints_img = data['joints_coord_img']\n",
    "            joints_img_xy1 = np.concatenate((joints_img[:,:2], np.ones_like(joints_img[:,:1])),1)\n",
    "            joints_img = np.dot(img2bb_trans, joints_img_xy1.transpose(1,0)).transpose(1,0)[:,:2]\n",
    "            if not cfg.simcc:\n",
    "                joints_img_copy = joints_img.copy()\n",
    "                ## normalize to [0,1]\n",
    "                joints_img_copy[:,0] /= cfg.input_img_shape[0]\n",
    "                joints_img_copy[:,1] /= cfg.input_img_shape[1]\n",
    "                targets['joints_img'] = joints_img_copy\n",
    "            else:\n",
    "                targets['joints_img'] = joints_img\n",
    "            \n",
    "            skeleton_map = skeleton_map_gray((cfg.input_img_shape[0], cfg.input_img_shape[1]), joints_img)\n",
    "            skeleton_map = self.transform(skeleton_map.astype(np.float32))/255.\n",
    "\n",
    "            inputs = {'img': img}\n",
    "            targets['skeleton_map'] = skeleton_map\n",
    "        else:\n",
    "            inputs = {'img': img}\n",
    "            targets = {}\n",
    "\n",
    "        return inputs, targets\n",
    "                  \n",
    "    def evaluate(self, outs, cur_sample_idx):\n",
    "        annots = self.datalist\n",
    "        sample_num = len(outs)\n",
    "        for n in range(sample_num):            \n",
    "            annot = annots[cur_sample_idx + n]\n",
    "            # cv2.namedWindow(annot['img_path'], 0)\n",
    "            \n",
    "            out = outs[n]\n",
    "            \n",
    "            # img convert\n",
    "            img = load_img(annot['img_path'])\n",
    "            orig_img = copy.deepcopy(img)\n",
    "            img, img2bb_trans, bb2img_trans, rot, scale = augmentation(img, annot['bbox'], self.data_split, do_flip=False)\n",
    "        \n",
    "    #         # GT and out['keypoints]\n",
    "            gt_joints_coord_img = annot['joints_coord_img']\n",
    "            joints_img_xy1 = np.concatenate((gt_joints_coord_img[:,:2], np.ones_like(gt_joints_coord_img[:,:1])),1)\n",
    "            joints_img = np.dot(img2bb_trans, joints_img_xy1.transpose(1,0)).transpose(1,0)[:,:2]\n",
    "            \n",
    "            if cfg.backbone == 'unext':\n",
    "                gt_skeleton_map = skeleton_map_gray((cfg.input_img_shape[0], cfg.input_img_shape[1]), joints_img)\n",
    "                gt_skeleton_map = gt_skeleton_map/255.\n",
    "                \n",
    "                pred_skeleton_map = (out['skeleton_map'].squeeze().cpu().numpy()).astype(float)# > 0.5\n",
    "                \n",
    "                ## show result\n",
    "                cat_imgs = ux_hon_result(orig_img, img, pred_skeleton_map, gt_skeleton_map)\n",
    "                cat_imgs = ux_hon_result_final(out, bb2img_trans, orig_img, img, cat_imgs)\n",
    "                \n",
    "                parts = re.split(r'[\\\\/]', annot['img_path'])\n",
    "                path = osp.join(cfg.vis_dir,'_'.join(parts[1:]))\n",
    "                cv2.imwrite(path, cat_imgs)\n",
    "                # cv2.imshow(annot['img_path'], cat_imgs)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "\n",
    "                pred_skeleton_map = (out['skeleton_map'].squeeze().cpu().numpy()>0.5).astype(float)# > 0.5\n",
    " \n",
    "                num_correct = (pred_skeleton_map==gt_skeleton_map).sum()\n",
    "                num_pixels = cfg.input_img_shape[0] * cfg.input_img_shape[1]\n",
    "            else:\n",
    "                img_uint8 = cv2.resize(orig_img.astype(np.uint8), (cfg.input_img_shape[0], cfg.input_img_shape[1]))\n",
    "                rgb_img_uint8 = cv2.cvtColor(img_uint8.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                rgb_img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                ori_imgs = np.hstack([rgb_img_uint8, rgb_img])\n",
    "                cat_imgs = ux_hon_result_final(out, bb2img_trans, orig_img, img, ori_imgs)\n",
    "                \n",
    "                parts = re.split(r'[\\\\/]', annot['img_path'])\n",
    "                path = osp.join(cfg.vis_dir,'_'.join(parts[1:]))\n",
    "                cv2.imwrite(path, cat_imgs)\n",
    "                # cv2.imshow(annot['img_path'], cat_imgs)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "\n",
    "            if cfg.simcc:\n",
    "                pred_keypoints = np.expand_dims(out['keypoints'], axis=0)\n",
    "                pred_keypoint_scores = out['keypoint_scores']\n",
    "                keypoints_restored = np.dot(bb2img_trans, np.concatenate((pred_keypoints[0], np.ones((pred_keypoints[0].shape[0], 1))), axis=1).transpose(1, 0))\n",
    "                keypoints_restored = keypoints_restored[:2, :].transpose(1, 0)\n",
    "            else:\n",
    "                pred_keypoints = np.expand_dims(out['joints_coord_img'].cpu().numpy(), axis=0)\n",
    "                pred_keypoints[:,:,0] *= cfg.input_img_shape[1]\n",
    "                pred_keypoints[:,:,1] *= cfg.input_img_shape[0]\n",
    "                keypoints_restored = np.dot(bb2img_trans, np.concatenate((pred_keypoints[0], np.ones((pred_keypoints[0].shape[0], 1))), axis=1).transpose(1, 0))\n",
    "                keypoints_restored = keypoints_restored[:2, :].transpose(1, 0)\n",
    "                pred_keypoint_scores = np.any(keypoints_restored, axis=1)\n",
    "\n",
    "    #         # flip back to left hand\n",
    "    #         if annot['hand_type'] == 'left':\n",
    "    #             joints_out[:,0] *= -1 \n",
    "            _, avg_acc, _ = keypoint_pck_accuracy(\n",
    "                pred=np.expand_dims(keypoints_restored, axis=0),\n",
    "                gt=np.expand_dims(gt_joints_coord_img[:,:2], axis=0),\n",
    "                mask=np.expand_dims(pred_keypoint_scores, axis=0) > 0,\n",
    "                thr=cfg.pck_thr,\n",
    "                norm_factor=np.expand_dims(annot['img_shape'], axis=0),\n",
    "            )\n",
    "\n",
    "            self.eval_result[2].append(compute_mpjpe(keypoints_restored, gt_joints_coord_img[:,:2]))\n",
    "\n",
    "            if cfg.backbone == 'unext':\n",
    "                self.eval_result[0].append(num_correct / num_pixels)\n",
    "                self.eval_result[1].append(avg_acc)\n",
    "            else:\n",
    "                self.eval_result[0].append(avg_acc)\n",
    "                \n",
    "    def print_eval_result(self, test_epoch):\n",
    "        message = []\n",
    "        if cfg.backbone == 'unext':\n",
    "            message.append('Output: {0}, Model: snapshot_{1}.pth.tar'.format(cfg.output_dir.split('\\\\')[-1], test_epoch))\n",
    "            message.append('Correct/Total(One Batch) pixels: {0:.2f}'.format(np.mean(self.eval_result[0]) * 100))\n",
    "            message.append('PCK@{0}: {1:.2f}'.format(cfg.pck_thr, np.mean(self.eval_result[1]) * 100))\n",
    "            message.append('MPJPE : %.2f' % np.mean(self.eval_result[2]))\n",
    "        else:\n",
    "            message.append('Output: {0}, Model: snapshot_{1}.pth.tar'.format(cfg.output_dir.split('\\\\')[-1], test_epoch))\n",
    "            message.append('PCK@{0}: {1:.2f}'.format(cfg.pck_thr, np.mean(self.eval_result[0]) * 100))\n",
    "            message.append('MPJPE : %.2f' % np.mean(self.eval_result[2]))\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base\n",
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import abc\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim\n",
    "import torchvision.transforms as transforms\n",
    "from common.timer import Timer\n",
    "from common.logger import colorlogger\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "from main.config import cfg\n",
    "# dynamic model import\n",
    "if cfg.backbone == 'fpn':\n",
    "    from main.model import get_model\n",
    "\n",
    "# dynamic dataset import\n",
    "# exec('from ' + cfg.trainset + ' import ' + cfg.trainset)\n",
    "# exec('from ' + cfg.testset + ' import ' + cfg.testset)\n",
    "\n",
    "class Base(object):\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    def __init__(self, log_name='logs.txt'):\n",
    "        \n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        # timer\n",
    "        self.tot_timer = Timer()\n",
    "        self.gpu_timer = Timer()\n",
    "        self.read_timer = Timer()\n",
    "\n",
    "        # logger\n",
    "        self.logger = colorlogger(cfg.log_dir, log_name=log_name)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _make_batch_generator(self):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _make_model(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HandOccNet Trainer\n",
    "class Trainer(Base):\n",
    "    def __init__(self):\n",
    "        super(Trainer, self).__init__(log_name = 'train_logs.txt')\n",
    "\n",
    "    def get_optimizer(self, model):\n",
    "        model_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        optimizer = torch.optim.Adam(model_params, lr=cfg.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def save_model(self, state, epoch):\n",
    "        file_path = osp.join(cfg.model_dir,'snapshot_{}.pth.tar'.format(str(epoch)))\n",
    "        torch.save(state, file_path)\n",
    "        self.logger.info(\"Write snapshot into {}\".format(file_path))\n",
    "\n",
    "    def load_model(self, model, optimizer):\n",
    "        model_file_list = glob.glob(osp.join(cfg.model_dir,'*.pth.tar'))\n",
    "        cur_epoch = max([int(file_name[file_name.find('snapshot_') + 9 : file_name.find('.pth.tar')]) for file_name in model_file_list])\n",
    "        ckpt_path = osp.join(cfg.model_dir, 'snapshot_' + str(cur_epoch) + '.pth.tar')\n",
    "        ckpt = torch.load(ckpt_path) \n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        #optimizer.load_state_dict(ckpt['optimizer'])\n",
    "\n",
    "        self.logger.info('Load checkpoint from {}'.format(ckpt_path))\n",
    "        return start_epoch, model, optimizer\n",
    "\n",
    "    def set_lr(self, epoch):\n",
    "        for e in cfg.lr_dec_epoch:\n",
    "            if epoch < e:\n",
    "                break\n",
    "        if epoch < cfg.lr_dec_epoch[-1]:\n",
    "            idx = cfg.lr_dec_epoch.index(e)\n",
    "            for g in self.optimizer.param_groups:\n",
    "                g['lr'] = cfg.lr * (cfg.lr_dec_factor ** idx)\n",
    "        else:\n",
    "            for g in self.optimizer.param_groups:\n",
    "                g['lr'] = cfg.lr * (cfg.lr_dec_factor ** len(cfg.lr_dec_epoch))\n",
    "\n",
    "    def get_lr(self):\n",
    "        for g in self.optimizer.param_groups:\n",
    "            cur_lr = g['lr']\n",
    "        return cur_lr\n",
    "    \n",
    "    def _make_batch_generator(self):\n",
    "        # data load and construct batch generator\n",
    "        self.logger.info(\"Creating dataset...\")\n",
    "        # Augment train data\n",
    "        # train_transforms = transforms.Compose([\n",
    "        #     transforms.Resize((256, 192)),\n",
    "        #     transforms.ToTensor()\n",
    "        # ])\n",
    "        train_dataset = eval(cfg.trainset)(transforms.ToTensor(), \"train\")\n",
    "            \n",
    "        self.itr_per_epoch = math.ceil(len(train_dataset) / cfg.num_gpus / cfg.train_batch_size)\n",
    "        self.batch_generator = DataLoader(dataset=train_dataset, batch_size=cfg.num_gpus*cfg.train_batch_size, shuffle=True, num_workers=cfg.num_thread, pin_memory=True)\n",
    "\n",
    "    def _make_model(self):\n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph and optimizer...\")\n",
    "        if cfg.SET:\n",
    "            self.logger.info(\"Creating model with SET...\")\n",
    "        else:\n",
    "            self.logger.info(\"Creating model without SET...\")\n",
    "        model = get_model('train')\n",
    "\n",
    "        model = DataParallel(model).cuda()\n",
    "        optimizer = self.get_optimizer(model)\n",
    "        if cfg.continue_train:\n",
    "            start_epoch, model, optimizer = self.load_model(model, optimizer)\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        model.train()\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HandOccNet Tester\n",
    "class Tester(Base):\n",
    "    def __init__(self):\n",
    "        super(Tester, self).__init__(log_name = 'test_logs.txt')\n",
    "\n",
    "    def _make_batch_generator(self):\n",
    "        # data load and construct batch generator\n",
    "        self.logger.info(\"Creating dataset...\")\n",
    "        # Augment train data\n",
    "        # test_transforms = transforms.Compose([\n",
    "        #     transforms.Resize((256, 192)),\n",
    "        #     transforms.ToTensor()\n",
    "        # ])\n",
    "        self.test_dataset = eval(cfg.testset)(transforms.ToTensor(), \"test\")\n",
    "        self.batch_generator = DataLoader(dataset=self.test_dataset, batch_size=cfg.num_gpus*cfg.test_batch_size, shuffle=False, num_workers=cfg.num_thread, pin_memory=True)\n",
    "       \n",
    "    def _make_model(self, test_epoch):\n",
    "        model_path = os.path.join(cfg.model_dir, 'snapshot_%d.pth.tar' % test_epoch)\n",
    "        assert os.path.exists(model_path), 'Cannot find model at ' + model_path\n",
    "        self.logger.info('Load checkpoint from {}'.format(model_path))\n",
    "        \n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph...\")\n",
    "        model = get_model('test')\n",
    "        model = DataParallel(model).cuda()\n",
    "        ckpt = torch.load(model_path)\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        model.eval()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def _evaluate(self, outs, cur_sample_idx):\n",
    "        eval_result = self.test_dataset.evaluate(outs, cur_sample_idx)\n",
    "        return eval_result\n",
    "\n",
    "    def _print_eval_result(self, test_epoch):\n",
    "        message = self.test_dataset.print_eval_result(test_epoch)\n",
    "        for msg in message:\n",
    "            self.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HandOccNet train + test process\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from main.config import cfg\n",
    "\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer._make_batch_generator()\n",
    "trainer._make_model()\n",
    "\n",
    "tester = Tester()\n",
    "tester._make_batch_generator()\n",
    "\n",
    "# train\n",
    "for epoch in range(trainer.start_epoch, cfg.end_epoch):\n",
    "    \n",
    "    trainer.set_lr(epoch)\n",
    "    trainer.tot_timer.tic()\n",
    "    trainer.read_timer.tic()\n",
    "    for itr, (inputs, targets) in enumerate(trainer.batch_generator):\n",
    "        trainer.read_timer.toc()\n",
    "        trainer.gpu_timer.tic()\n",
    "\n",
    "        # forward\n",
    "        trainer.optimizer.zero_grad()\n",
    "        if cfg.simcc:\n",
    "            loss, acc = trainer.model(inputs, targets, 'train')\n",
    "        else:\n",
    "            loss = trainer.model(inputs, targets, 'train')\n",
    "\n",
    "        loss = {k:loss[k].mean() for k in loss}\n",
    "\n",
    "        # backward\n",
    "        sum(loss[k] for k in loss).backward()\n",
    "        trainer.optimizer.step()\n",
    "        trainer.gpu_timer.toc()\n",
    "        screen = [\n",
    "            'Epoch %d/%d itr %d/%d:' % (epoch, cfg.end_epoch, itr, trainer.itr_per_epoch),\n",
    "            'lr: %g' % (trainer.get_lr()),\n",
    "            'speed: %.2f(gpu%.2fs r_data%.2fs)s/itr' % (\n",
    "                trainer.tot_timer.average_time, trainer.gpu_timer.average_time, trainer.read_timer.average_time),\n",
    "            '%.2fh/epoch' % (trainer.tot_timer.average_time / 3600. * trainer.itr_per_epoch),\n",
    "            ]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in loss.items()]\n",
    "        if cfg.backbone == 'crossatt' or cfg.simcc:\n",
    "            screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in acc.items()]\n",
    "        trainer.logger.info(' '.join(screen))\n",
    "\n",
    "        trainer.tot_timer.toc()\n",
    "        trainer.tot_timer.tic()\n",
    "        trainer.read_timer.tic()\n",
    "    \n",
    "    if (epoch+1)%cfg.ckpt_freq== 0 or epoch+1 == cfg.end_epoch:\n",
    "        trainer.save_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.model.state_dict(),\n",
    "            'optimizer': trainer.optimizer.state_dict(),\n",
    "        }, epoch+1)\n",
    "\n",
    "        tester._make_model(epoch+1)\n",
    "\n",
    "        eval_result = {}\n",
    "        cur_sample_idx = 0\n",
    "        for itr, (inputs, targets) in enumerate(tqdm(tester.batch_generator)):\n",
    "            \n",
    "            # forward\n",
    "            with torch.no_grad():\n",
    "                out = tester.model(inputs, targets, 'test')\n",
    "            \n",
    "            # save output\n",
    "            out = {k: v for k,v in out.items()}\n",
    "            for k,v in out.items(): batch_size = out[k].shape[0]\n",
    "            out = [{k: v[bid] for k,v in out.items()} for bid in range(batch_size)]\n",
    "\n",
    "            # evaluate\n",
    "            tester._evaluate(out, cur_sample_idx)\n",
    "            cur_sample_idx += len(out)\n",
    "\n",
    "        tester._print_eval_result(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HandOccNet train process\n",
    "\n",
    "import argparse\n",
    "from main.config import cfg\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer._make_batch_generator()\n",
    "trainer._make_model()\n",
    "\n",
    "# train\n",
    "for epoch in range(trainer.start_epoch, cfg.end_epoch):\n",
    "    \n",
    "    trainer.set_lr(epoch)\n",
    "    trainer.tot_timer.tic()\n",
    "    trainer.read_timer.tic()\n",
    "    for itr, (inputs, targets) in enumerate(trainer.batch_generator):\n",
    "        trainer.read_timer.toc()\n",
    "        trainer.gpu_timer.tic()\n",
    "\n",
    "        # forward\n",
    "        trainer.optimizer.zero_grad()\n",
    "        if cfg.simcc:\n",
    "            loss, acc = trainer.model(inputs, targets, 'train')\n",
    "        else:\n",
    "            loss = trainer.model(inputs, targets, 'train')\n",
    "\n",
    "        loss = {k:loss[k].mean() for k in loss}\n",
    "\n",
    "        # backward\n",
    "        sum(loss[k] for k in loss).backward()\n",
    "        trainer.optimizer.step()\n",
    "        trainer.gpu_timer.toc()\n",
    "        screen = [\n",
    "            'Epoch %d/%d itr %d/%d:' % (epoch, cfg.end_epoch, itr, trainer.itr_per_epoch),\n",
    "            'lr: %g' % (trainer.get_lr()),\n",
    "            'speed: %.2f(gpu%.2fs r_data%.2fs)s/itr' % (\n",
    "                trainer.tot_timer.average_time, trainer.gpu_timer.average_time, trainer.read_timer.average_time),\n",
    "            '%.2fh/epoch' % (trainer.tot_timer.average_time / 3600. * trainer.itr_per_epoch),\n",
    "            ]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in loss.items()]\n",
    "        if cfg.simcc:\n",
    "            screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in acc.items()]\n",
    "        trainer.logger.info(' '.join(screen))\n",
    "\n",
    "        trainer.tot_timer.toc()\n",
    "        trainer.tot_timer.tic()\n",
    "        trainer.read_timer.tic()\n",
    "    \n",
    "    if (epoch+1)%cfg.ckpt_freq== 0 or epoch+1 == cfg.end_epoch:\n",
    "        trainer.save_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.model.state_dict(),\n",
    "            'optimizer': trainer.optimizer.state_dict(),\n",
    "        }, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HandOccNet test process\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from main.config import cfg\n",
    "\n",
    "test_epoch = [epoch for epoch in range(10, 71, 10)]\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "tester = Tester()\n",
    "tester._make_batch_generator()\n",
    "\n",
    "for epoch in test_epoch:\n",
    "    tester._make_model(epoch)\n",
    "\n",
    "    eval_result = {}\n",
    "    cur_sample_idx = 0\n",
    "    for itr, (inputs, targets) in enumerate(tqdm(tester.batch_generator)):\n",
    "        \n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            out = tester.model(inputs, targets, 'test')\n",
    "        \n",
    "        # save output\n",
    "        out = {k: v for k,v in out.items()}\n",
    "        for k,v in out.items(): batch_size = out[k].shape[0]\n",
    "        out = [{k: v[bid] for k,v in out.items()} for bid in range(batch_size)]\n",
    "\n",
    "        # evaluate\n",
    "        tester._evaluate(out, cur_sample_idx)\n",
    "        cur_sample_idx += len(out)\n",
    "\n",
    "    tester._print_eval_result(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UneXt + HandOccNet UH_Trainer\n",
    "from main.model_UX import get_UX_model\n",
    "from main.model_HON import get_HON_model\n",
    "\n",
    "class UH_Trainer(Base):\n",
    "    def __init__(self):\n",
    "        super(UH_Trainer, self).__init__(log_name = 'train_logs.txt')\n",
    "\n",
    "    def get_UX_optimizer(self, model):\n",
    "        # model_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.ux_lr)\n",
    "        return optimizer\n",
    "\n",
    "    def get_HON_optimizer(self, model):\n",
    "        # model_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.hon_lr)\n",
    "        return optimizer\n",
    "\n",
    "    def save_UX_model(self, state, epoch):\n",
    "        file_path = osp.join(cfg.model_dir,'snapshot_UX_{}.pth.tar'.format(str(epoch)))\n",
    "        torch.save(state, file_path)\n",
    "        self.logger.info(\"Write UX snapshot into {}\".format(file_path))\n",
    "\n",
    "    def save_HON_model(self, state, epoch):\n",
    "        file_path = osp.join(cfg.model_dir,'snapshot_HON_{}.pth.tar'.format(str(epoch)))\n",
    "        torch.save(state, file_path)\n",
    "        self.logger.info(\"Write HON snapshot into {}\".format(file_path))\n",
    "\n",
    "    def load_UX_model(self, model, optimizer):\n",
    "        model_file_list = glob.glob(osp.join(cfg.model_dir,'*.pth.tar'))\n",
    "        cur_epoch = max([int(file_name[file_name.find('snapshot_UX_') + 9 : file_name.find('.pth.tar')]) for file_name in model_file_list])\n",
    "        ckpt_path = osp.join(cfg.model_dir, 'snapshot_UX_' + str(cur_epoch) + '.pth.tar')\n",
    "        ckpt = torch.load(ckpt_path) \n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        #optimizer.load_state_dict(ckpt['optimizer'])\n",
    "\n",
    "        self.logger.info('Load checkpoint from {}'.format(ckpt_path))\n",
    "        return start_epoch, model, optimizer\n",
    "\n",
    "    def load_HON_model(self, model, optimizer):\n",
    "        model_file_list = glob.glob(osp.join(cfg.model_dir,'*.pth.tar'))\n",
    "        cur_epoch = max([int(file_name[file_name.find('snapshot_HON_') + 9 : file_name.find('.pth.tar')]) for file_name in model_file_list])\n",
    "        ckpt_path = osp.join(cfg.model_dir, 'snapshot_HON_' + str(cur_epoch) + '.pth.tar')\n",
    "        ckpt = torch.load(ckpt_path) \n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        #optimizer.load_state_dict(ckpt['optimizer'])\n",
    "\n",
    "        self.logger.info('Load checkpoint from {}'.format(ckpt_path))\n",
    "        return start_epoch, model, optimizer\n",
    "    \n",
    "    def set_ux_lr(self, epoch):\n",
    "        for e in cfg.lr_dec_epoch:\n",
    "            if epoch < e:\n",
    "                break\n",
    "        if epoch < cfg.lr_dec_epoch[-1]:\n",
    "            idx = cfg.lr_dec_epoch.index(e)\n",
    "            for g in self.ux_optimizer.param_groups:\n",
    "                g['lr'] = cfg.ux_lr * (cfg.lr_dec_factor ** idx)\n",
    "        else:\n",
    "            for g in self.ux_optimizer.param_groups:\n",
    "                g['lr'] = cfg.ux_lr * (cfg.lr_dec_factor ** len(cfg.lr_dec_epoch))\n",
    "\n",
    "    def set_hon_lr(self, epoch):\n",
    "        for e in cfg.lr_dec_epoch:\n",
    "            if epoch < e:\n",
    "                break\n",
    "        if epoch < cfg.lr_dec_epoch[-1]:\n",
    "            idx = cfg.lr_dec_epoch.index(e)\n",
    "            for g in self.hon_optimizer.param_groups:\n",
    "                g['lr'] = cfg.hon_lr * (cfg.lr_dec_factor ** idx)\n",
    "        else:\n",
    "            for g in self.hon_optimizer.param_groups:\n",
    "                g['lr'] = cfg.hon_lr * (cfg.lr_dec_factor ** len(cfg.lr_dec_epoch))\n",
    "\n",
    "    def get_ux_lr(self):\n",
    "        for g in self.ux_optimizer.param_groups:\n",
    "            cur_lr = g['lr']\n",
    "        return cur_lr\n",
    "    \n",
    "    def get_hon_lr(self):\n",
    "        for g in self.hon_optimizer.param_groups:\n",
    "            cur_lr = g['lr']\n",
    "        return cur_lr\n",
    "    \n",
    "    def _make_batch_generator(self):\n",
    "        # data load and construct batch generator\n",
    "        self.logger.info(\"Creating dataset...\")\n",
    "        # Augment train data\n",
    "        # train_transforms = transforms.Compose([\n",
    "        #     transforms.Resize((256, 192)),\n",
    "        #     transforms.ToTensor()\n",
    "        # ])\n",
    "        train_dataset = eval(cfg.trainset)(transforms.ToTensor(), \"train\")\n",
    "            \n",
    "        self.itr_per_epoch = math.ceil(len(train_dataset) / cfg.num_gpus / cfg.train_batch_size)\n",
    "        self.batch_generator = DataLoader(dataset=train_dataset, batch_size=cfg.num_gpus*cfg.train_batch_size, shuffle=True, num_workers=cfg.num_thread, pin_memory=True)\n",
    "\n",
    "    def _make_UX_model(self):\n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph and optimizer...\")\n",
    "        model = get_UX_model('train')\n",
    "\n",
    "        model = DataParallel(model).cuda()\n",
    "        ux_optimizer = self.get_UX_optimizer(model)\n",
    "        if cfg.continue_train:\n",
    "            start_epoch, model, ux_optimizer = self.load_model(model, ux_optimizer)\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        model.train()\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.ux_model = model\n",
    "        self.ux_optimizer = ux_optimizer\n",
    "\n",
    "    def _make_HON_model(self):\n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph and optimizer...\")\n",
    "        if cfg.SET:\n",
    "            self.logger.info(\"Creating model with SET...\")\n",
    "        else:\n",
    "            self.logger.info(\"Creating model without SET...\")\n",
    "        model = get_HON_model('train')\n",
    "\n",
    "        model = DataParallel(model).cuda()\n",
    "        hon_optimizer = self.get_HON_optimizer(model)\n",
    "        if cfg.continue_train:\n",
    "            start_epoch, model, hon_optimizer = self.load_model(model, hon_optimizer)\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        model.train()\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.hon_model = model\n",
    "        self.hon_optimizer = hon_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UneXt + HandOccNet UH_Tester\n",
    "from main.model_UX import get_UX_model\n",
    "from main.model_HON import get_HON_model\n",
    "\n",
    "class UH_Tester(Base):\n",
    "    def __init__(self):\n",
    "        super(UH_Tester, self).__init__(log_name = 'test_logs.txt')\n",
    "\n",
    "    def _make_batch_generator(self):\n",
    "        # data load and construct batch generator\n",
    "        self.logger.info(\"Creating dataset...\")\n",
    "        # Augment train data\n",
    "        # test_transforms = transforms.Compose([\n",
    "        #     transforms.Resize((256, 192)),\n",
    "        #     transforms.ToTensor()\n",
    "        # ])\n",
    "        self.test_dataset = eval(cfg.testset)(transforms.ToTensor(), \"test\")\n",
    "        self.batch_generator = DataLoader(dataset=self.test_dataset, batch_size=cfg.num_gpus*cfg.test_batch_size, shuffle=False, num_workers=cfg.num_thread, pin_memory=True)\n",
    "       \n",
    "    def _make_UX_model(self, test_epoch):\n",
    "        model_path = os.path.join(cfg.model_dir, 'snapshot_UX_%d.pth.tar' % test_epoch)\n",
    "        assert os.path.exists(model_path), 'Cannot find model at ' + model_path\n",
    "        self.logger.info('Load checkpoint from {}'.format(model_path))\n",
    "        \n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph...\")\n",
    "        model = get_UX_model('test')\n",
    "        model = DataParallel(model).cuda()\n",
    "        ckpt = torch.load(model_path)\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        model.eval()\n",
    "\n",
    "        self.ux_model = model\n",
    "\n",
    "    def _make_HON_model(self, test_epoch):\n",
    "        model_path = os.path.join(cfg.model_dir, 'snapshot_HON_%d.pth.tar' % test_epoch)\n",
    "        assert os.path.exists(model_path), 'Cannot find model at ' + model_path\n",
    "        self.logger.info('Load checkpoint from {}'.format(model_path))\n",
    "        \n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph...\")\n",
    "        model = get_HON_model('test')\n",
    "        model = DataParallel(model).cuda()\n",
    "        ckpt = torch.load(model_path)\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        model.eval()\n",
    "\n",
    "        self.hon_model = model\n",
    "\n",
    "    def _evaluate(self, outs, cur_sample_idx):\n",
    "        eval_result = self.test_dataset.evaluate(outs, cur_sample_idx)\n",
    "        return eval_result\n",
    "\n",
    "    def _print_eval_result(self, test_epoch):\n",
    "        message = self.test_dataset.print_eval_result(test_epoch)\n",
    "        for msg in message:\n",
    "            self.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unext + HandOccNet train + test process\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from main.config import cfg\n",
    "\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "trainer = UH_Trainer()\n",
    "trainer._make_batch_generator()\n",
    "trainer._make_UX_model()\n",
    "trainer._make_HON_model()\n",
    "\n",
    "tester = UH_Tester()\n",
    "tester._make_batch_generator()\n",
    "\n",
    "# train\n",
    "for epoch in range(trainer.start_epoch, cfg.end_epoch):\n",
    "    \n",
    "    trainer.set_ux_lr(epoch)\n",
    "    trainer.set_hon_lr(epoch)\n",
    "    trainer.tot_timer.tic()\n",
    "    trainer.read_timer.tic()\n",
    "    for itr, (inputs, targets) in enumerate(trainer.batch_generator):\n",
    "        trainer.read_timer.toc()\n",
    "        trainer.gpu_timer.tic()\n",
    "\n",
    "        ux_loss, ux_acc, ux_outs = trainer.ux_model(inputs, targets, 'train', itr)\n",
    "\n",
    "        ux_loss_dic = {k:ux_loss[k].mean() for k in ux_loss}\n",
    "        ux_loss = sum(ux_loss[k] for k in ux_loss_dic)\n",
    "\n",
    "        # ux forward\n",
    "        trainer.ux_optimizer.zero_grad()\n",
    "        # ux backward\n",
    "        ux_loss.backward(retain_graph=True)\n",
    "        trainer.ux_optimizer.step()\n",
    "\n",
    "        if cfg.simcc:\n",
    "            hon_loss, hon_acc = trainer.hon_model(inputs, targets, ux_outs, 'train')\n",
    "        else:\n",
    "            hon_loss = trainer.hon_model(inputs, targets, 'train')\n",
    "\n",
    "        hon_loss_dic = {k:hon_loss[k].mean() for k in hon_loss}\n",
    "        hon_loss = sum(hon_loss[k] for k in hon_loss_dic)\n",
    "\n",
    "        # hon forward\n",
    "        trainer.hon_optimizer.zero_grad()\n",
    "        # hon backward\n",
    "        hon_loss.backward()\n",
    "        trainer.hon_optimizer.step()\n",
    "\n",
    "        trainer.gpu_timer.toc()\n",
    "        screen = [\n",
    "            'Epoch %d/%d itr %d/%d:' % (epoch, cfg.end_epoch, itr, trainer.itr_per_epoch),\n",
    "            'ux_lr: %g' % (trainer.get_ux_lr()),\n",
    "            'hon_lr: %g' % (trainer.get_hon_lr()),\n",
    "            'speed: %.2f(gpu%.2fs r_data%.2fs)s/itr' % (\n",
    "                trainer.tot_timer.average_time, trainer.gpu_timer.average_time, trainer.read_timer.average_time),\n",
    "            '%.2fh/epoch' % (trainer.tot_timer.average_time / 3600. * trainer.itr_per_epoch),\n",
    "            ]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in ux_loss_dic.items()]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in hon_loss_dic.items()]\n",
    "        \n",
    "        screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in ux_acc.items()]\n",
    "        if cfg.simcc:\n",
    "            screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in hon_acc.items()]\n",
    "        trainer.logger.info(' '.join(screen))\n",
    "\n",
    "        trainer.tot_timer.toc()\n",
    "        trainer.tot_timer.tic()\n",
    "        trainer.read_timer.tic()\n",
    "    \n",
    "    if (epoch+1)%cfg.ckpt_freq== 0 or epoch+1 == cfg.end_epoch:\n",
    "        trainer.save_UX_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.ux_model.state_dict(),\n",
    "            'optimizer': trainer.ux_optimizer.state_dict(),\n",
    "        }, epoch+1)\n",
    "\n",
    "        trainer.save_HON_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.hon_model.state_dict(),\n",
    "            'optimizer': trainer.hon_optimizer.state_dict(),\n",
    "        }, epoch+1)\n",
    "\n",
    "        tester._make_UX_model(epoch+1)\n",
    "        tester._make_HON_model(epoch+1)\n",
    "\n",
    "        eval_result = {}\n",
    "        cur_sample_idx = 0\n",
    "        for itr, (inputs, targets) in enumerate(tqdm(tester.batch_generator)):\n",
    "            \n",
    "            # forward\n",
    "            with torch.no_grad():\n",
    "                ux_out = tester.ux_model(inputs, targets, 'test', itr)\n",
    "                hon_out = tester.hon_model(inputs, targets, ux_out, 'test')\n",
    "            \n",
    "            # save output\n",
    "            ux_out = {k: v for k,v in ux_out.items()}\n",
    "            for k,v in ux_out.items(): batch_size = ux_out[k].shape[0]\n",
    "            hon_out = {k: v for k,v in hon_out.items()}\n",
    "            for k,v in hon_out.items(): batch_size = hon_out[k].shape[0]\n",
    "            out = []\n",
    "            for bid in range(batch_size):\n",
    "                combined_dict = {}\n",
    "                for k,v in ux_out.items():\n",
    "                    combined_dict[k] = v[bid]\n",
    "                for k,v in hon_out.items():\n",
    "                    combined_dict[k] = v[bid]\n",
    "                out.append(combined_dict)\n",
    "                \n",
    "            # evaluate\n",
    "            tester._evaluate(out, cur_sample_idx)\n",
    "            cur_sample_idx += len(out)\n",
    "\n",
    "        tester._print_eval_result(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unext + HandOccNet train process\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from main.config import cfg\n",
    "\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "trainer = UH_Trainer()\n",
    "trainer._make_batch_generator()\n",
    "trainer._make_UX_model()\n",
    "trainer._make_HON_model()\n",
    "\n",
    "# train\n",
    "for epoch in range(trainer.start_epoch, cfg.end_epoch):\n",
    "    \n",
    "    trainer.set_ux_lr(epoch)\n",
    "    trainer.set_hon_lr(epoch)\n",
    "    trainer.tot_timer.tic()\n",
    "    trainer.read_timer.tic()\n",
    "    for itr, (inputs, targets) in enumerate(trainer.batch_generator):\n",
    "        trainer.read_timer.toc()\n",
    "        trainer.gpu_timer.tic()\n",
    "\n",
    "        ux_loss, ux_acc, ux_outs = trainer.ux_model(inputs, targets, 'train', itr)\n",
    "\n",
    "        ux_loss_dic = {k:ux_loss[k].mean() for k in ux_loss}\n",
    "        ux_loss = sum(ux_loss[k] for k in ux_loss_dic)\n",
    "\n",
    "        # ux forward\n",
    "        trainer.ux_optimizer.zero_grad()\n",
    "        # ux backward\n",
    "        ux_loss.backward(retain_graph=True)\n",
    "        trainer.ux_optimizer.step()\n",
    "\n",
    "        if cfg.simcc:\n",
    "            hon_loss, hon_acc = trainer.hon_model(inputs, targets, ux_outs, 'train')\n",
    "        else:\n",
    "            hon_loss = trainer.hon_model(inputs, targets, 'train')\n",
    "\n",
    "        hon_loss_dic = {k:hon_loss[k].mean() for k in hon_loss}\n",
    "        hon_loss = sum(hon_loss[k] for k in hon_loss_dic)\n",
    "\n",
    "        # hon forward\n",
    "        trainer.hon_optimizer.zero_grad()\n",
    "        # hon backward\n",
    "        hon_loss.backward()\n",
    "        trainer.hon_optimizer.step()\n",
    "\n",
    "        trainer.gpu_timer.toc()\n",
    "        screen = [\n",
    "            'Epoch %d/%d itr %d/%d:' % (epoch, cfg.end_epoch, itr, trainer.itr_per_epoch),\n",
    "            'ux_lr: %g' % (trainer.get_ux_lr()),\n",
    "            'hon_lr: %g' % (trainer.get_hon_lr()),\n",
    "            'speed: %.2f(gpu%.2fs r_data%.2fs)s/itr' % (\n",
    "                trainer.tot_timer.average_time, trainer.gpu_timer.average_time, trainer.read_timer.average_time),\n",
    "            '%.2fh/epoch' % (trainer.tot_timer.average_time / 3600. * trainer.itr_per_epoch),\n",
    "            ]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in ux_loss_dic.items()]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in hon_loss_dic.items()]\n",
    "        \n",
    "        screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in ux_acc.items()]\n",
    "        if cfg.simcc:\n",
    "            screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in hon_acc.items()]\n",
    "        trainer.logger.info(' '.join(screen))\n",
    "\n",
    "        trainer.tot_timer.toc()\n",
    "        trainer.tot_timer.tic()\n",
    "        trainer.read_timer.tic()\n",
    "    \n",
    "    if (epoch+1)%cfg.ckpt_freq== 0 or epoch+1 == cfg.end_epoch:\n",
    "        trainer.save_UX_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.ux_model.state_dict(),\n",
    "            'optimizer': trainer.ux_optimizer.state_dict(),\n",
    "        }, epoch+1)\n",
    "\n",
    "        trainer.save_HON_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.hon_model.state_dict(),\n",
    "            'optimizer': trainer.hon_optimizer.state_dict(),\n",
    "        }, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unext + HandOccNet test process\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from main.config import cfg\n",
    "\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "tester = UH_Tester()\n",
    "tester._make_batch_generator()\n",
    "\n",
    "# train\n",
    "for epoch in range(9, cfg.end_epoch, 10):\n",
    "    tester._make_UX_model(epoch+1)\n",
    "    tester._make_HON_model(epoch+1)\n",
    "\n",
    "    eval_result = {}\n",
    "    cur_sample_idx = 0\n",
    "    for itr, (inputs, targets) in enumerate(tqdm(tester.batch_generator)):\n",
    "        \n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            ux_out = tester.ux_model(inputs, targets, 'test', itr)\n",
    "            hon_out = tester.hon_model(inputs, targets, ux_out, 'test')\n",
    "        \n",
    "        # save output\n",
    "        ux_out = {k: v for k,v in ux_out.items()}\n",
    "        for k,v in ux_out.items(): batch_size = ux_out[k].shape[0]\n",
    "        hon_out = {k: v for k,v in hon_out.items()}\n",
    "        for k,v in hon_out.items(): batch_size = hon_out[k].shape[0]\n",
    "        out = []\n",
    "        for bid in range(batch_size):\n",
    "            combined_dict = {}\n",
    "            for k,v in ux_out.items():\n",
    "                combined_dict[k] = v[bid]\n",
    "            for k,v in hon_out.items():\n",
    "                combined_dict[k] = v[bid]\n",
    "            out.append(combined_dict)\n",
    "            \n",
    "        # evaluate\n",
    "        tester._evaluate(out, cur_sample_idx)\n",
    "        cur_sample_idx += len(out)\n",
    "\n",
    "    tester._print_eval_result(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
