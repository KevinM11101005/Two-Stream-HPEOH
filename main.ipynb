{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEX_YCB\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import torchvision.transforms as transforms\n",
    "from common.logger import colorlogger\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from common.utils.preprocessing import load_img, get_bbox, process_bbox, generate_patch_image, augmentation\n",
    "from main.config import cfg\n",
    "from common.utils.transforms import compute_mpjpe, compute_pa_mpjpe\n",
    "from common.utils.skeleton_map import skeleton_map_gray\n",
    "from common.codecs.keypoint_eval import keypoint_pck_accuracy\n",
    "from mmpose.utils.tensor_utils import to_numpy\n",
    "from common.utils.vis import ux_hon_result, ux_hon_result_final\n",
    "\n",
    "class DEX_YCB(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, data_split, log_name='cfg_logs.txt'):\n",
    "        self.transform = transform\n",
    "        self.data_split = data_split if data_split == 'train' else 'test'\n",
    "        self.root_dir = osp.join('data', 'DEX_YCB')\n",
    "        self.annot_path = osp.join(self.root_dir, 'annotations')\n",
    "        self.hand_type = {'left': 0, 'right': 0}\n",
    "        self.datalist = self.load_data()\n",
    "        self.root_joint_idx = 0\n",
    "        if self.data_split != 'train':\n",
    "            self.eval_result = [[],[],[],[]] #[mpjpe_list, pa-mpjpe_list]\n",
    "        \n",
    "        self.logger = colorlogger(cfg.log_dir, log_name=log_name)\n",
    "        \n",
    "        if self.data_split == 'train':\n",
    "            for i in cfg.__dict__:\n",
    "                self.logger.info('{0}: {1}'.format(i, cfg.__dict__[i]))\n",
    "        \n",
    "        message = []\n",
    "        message.append(f\"DataList len: {len(self.datalist)}\")\n",
    "        message.append('left hand data: {0}, right hand data: {1}'.format(self.hand_type['left'], self.hand_type['right']))\n",
    "        \n",
    "        if cfg.simcc and cfg.SET:\n",
    "            message.append(f'Start the model {cfg.backbone} with SET and with simcc')\n",
    "        elif cfg.simcc:\n",
    "            message.append(f'Start the model {cfg.backbone} without SET and with simcc')\n",
    "        elif cfg.SET:\n",
    "            message.append(f'Start the model {cfg.backbone} with SET and with regressor')\n",
    "        else:\n",
    "            message.append(f'Start the model {cfg.backbone} without SET and with regressor')\n",
    "        for msg in message:\n",
    "            self.logger.info(msg)\n",
    "            \n",
    "    def load_data(self):\n",
    "        db = COCO(osp.join(self.annot_path, \"DEX_YCB_s0_{}_data.json\".format(self.data_split)))\n",
    "        \n",
    "        datalist = []\n",
    "        skip = 1\n",
    "\n",
    "        if self.data_split == 'train':\n",
    "            skip_mode = cfg.train_skip\n",
    "            remainder = cfg.train_remainder\n",
    "        elif self.data_split == 'test':\n",
    "            skip_mode = cfg.test_skip\n",
    "            remainder = cfg.test_remainder\n",
    "\n",
    "        for aid in db.anns.keys():\n",
    "            if skip % skip_mode == remainder:\n",
    "                ann = db.anns[aid]\n",
    "                image_id = ann['image_id']\n",
    "                img = db.loadImgs(image_id)[0]\n",
    "                if osp.exists(osp.join(self.root_dir, img['file_name'])):\n",
    "                    img_path = osp.join(self.root_dir, img['file_name'])\n",
    "                    img_shape = (img['height'], img['width'])\n",
    "                    \n",
    "                    joints_coord_img = np.array(ann['joints_img'], dtype=np.float32)\n",
    "                    hand_type = ann['hand_type']\n",
    "\n",
    "                    bbox = get_bbox(joints_coord_img[:,:2], np.ones_like(joints_coord_img[:,0]), expansion_factor=1.5)\n",
    "                    bbox = process_bbox(bbox, img['width'], img['height'], expansion_factor=1.0)\n",
    "\n",
    "                    data = {\"img_path\": img_path, \"img_shape\": img_shape, \"joints_coord_img\": joints_coord_img,\n",
    "                            \"bbox\": bbox, \"hand_type\": hand_type}\n",
    "                    \n",
    "                    if all(val is not None for val in data.values()):\n",
    "                        datalist.append(data)\n",
    "                        if data['hand_type'] == 'left':\n",
    "                            self.hand_type['left'] += 1\n",
    "                        else:\n",
    "                            self.hand_type['right'] += 1\n",
    "            skip += 1\n",
    "        return datalist\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = copy.deepcopy(self.datalist[idx])\n",
    "        img_path, img_shape, bbox = data['img_path'], data['img_shape'], data['bbox']\n",
    "        hand_type = data['hand_type']\n",
    "        do_flip = False # (hand_type == 'left')\n",
    "\n",
    "        # img\n",
    "        img = load_img(img_path)\n",
    "        orig_img = copy.deepcopy(img)[:,:,::-1]\n",
    "        img, img2bb_trans, bb2img_trans, rot, scale = augmentation(img, bbox, self.data_split, do_flip=do_flip)\n",
    "        # Convert numpy array to PIL Image\n",
    "        # img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        # img = Image.fromarray(img)\n",
    "        save_path = cfg.vis_dir + '/' + 'image'\n",
    "        save_path = save_path + '/' + str(idx) + '.jpg'\n",
    "        img = self.transform(img.astype(np.float32))/255.\n",
    "\n",
    "        if self.data_split == 'train':\n",
    "            targets = {}\n",
    "            ## 2D joint coordinate\n",
    "            joints_img = data['joints_coord_img']\n",
    "            # if do_flip:\n",
    "            #     joints_img[:,0] = img_shape[1] - joints_img[:,0] - 1\n",
    "            joints_img_xy1 = np.concatenate((joints_img[:,:2], np.ones_like(joints_img[:,:1])),1)\n",
    "            joints_img = np.dot(img2bb_trans, joints_img_xy1.transpose(1,0)).transpose(1,0)[:,:2]\n",
    "            if not cfg.simcc:\n",
    "                joints_img_copy = joints_img.copy()\n",
    "                ## normalize to [0,1]\n",
    "                joints_img_copy[:,0] /= cfg.input_img_shape[0]\n",
    "                joints_img_copy[:,1] /= cfg.input_img_shape[1]\n",
    "                targets['joints_img'] = joints_img_copy\n",
    "            else:\n",
    "                targets['joints_img'] = joints_img\n",
    "            \n",
    "            skeleton_map = skeleton_map_gray((cfg.input_img_shape[0], cfg.input_img_shape[1]), joints_img)\n",
    "            # cv2.imshow('test', skeleton_map)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "            skeleton_map = self.transform(skeleton_map.astype(np.float32))/255.\n",
    "\n",
    "            inputs = {'img': img}\n",
    "            targets['skeleton_map'] = skeleton_map\n",
    "        else:\n",
    "            inputs = {'img': img}\n",
    "            targets = {}\n",
    "\n",
    "        return inputs, targets\n",
    "    \n",
    "    def evaluate(self, outs, cur_sample_idx):\n",
    "        annots = self.datalist\n",
    "        sample_num = len(outs)\n",
    "        for n in range(sample_num):            \n",
    "            annot = annots[cur_sample_idx + n]\n",
    "            # cv2.namedWindow(annot['img_path'], 0)\n",
    "            \n",
    "            out = outs[n]\n",
    "            \n",
    "            # img convert\n",
    "            img = load_img(annot['img_path'])\n",
    "            orig_img = copy.deepcopy(img)\n",
    "            img, img2bb_trans, bb2img_trans, rot, scale = augmentation(img, annot['bbox'], self.data_split, do_flip=False)\n",
    "        \n",
    "    #         # GT and out['keypoints]\n",
    "            gt_joints_coord_img = annot['joints_coord_img']\n",
    "            joints_img_xy1 = np.concatenate((gt_joints_coord_img[:,:2], np.ones_like(gt_joints_coord_img[:,:1])),1)\n",
    "            joints_img = np.dot(img2bb_trans, joints_img_xy1.transpose(1,0)).transpose(1,0)[:,:2]\n",
    "            \n",
    "            if cfg.backbone == 'unext':\n",
    "                gt_skeleton_map = skeleton_map_gray((cfg.input_img_shape[0], cfg.input_img_shape[1]), joints_img)\n",
    "                gt_skeleton_map = gt_skeleton_map/255.\n",
    "                \n",
    "                pred_skeleton_map = (out['skeleton_map'].squeeze().cpu().numpy()).astype(float)# > 0.5\n",
    "                \n",
    "                ## show result\n",
    "                cat_imgs = ux_hon_result(orig_img, img, pred_skeleton_map, gt_skeleton_map)\n",
    "                cat_imgs = ux_hon_result_final(out, bb2img_trans, orig_img, img, cat_imgs)\n",
    "                \n",
    "                path = cfg.vis_dir+'/'+'_'.join(annot['img_path'].split('/'))\n",
    "                cv2.imwrite(path, cat_imgs)\n",
    "                # cv2.imshow(annot['img_path'], cat_imgs)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "\n",
    "                pred_skeleton_map = (out['skeleton_map'].squeeze().cpu().numpy()>0.5).astype(float)# > 0.5\n",
    " \n",
    "                num_correct = (pred_skeleton_map==gt_skeleton_map).sum()\n",
    "                num_pixels = cfg.input_img_shape[0] * cfg.input_img_shape[1]\n",
    "            else:\n",
    "                img_uint8 = cv2.resize(orig_img.astype(np.uint8), (cfg.input_img_shape[0], cfg.input_img_shape[1]))\n",
    "                rgb_img_uint8 = cv2.cvtColor(img_uint8.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                rgb_img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                ori_imgs = np.hstack([rgb_img_uint8, rgb_img])\n",
    "                cat_imgs = ux_hon_result_final(out, bb2img_trans, orig_img, img, ori_imgs)\n",
    "                \n",
    "                path = cfg.vis_dir+'/'+'_'.join(annot['img_path'].split('/'))\n",
    "                cv2.imwrite(path, cat_imgs)\n",
    "                # cv2.imshow(annot['img_path'], cat_imgs)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "                \n",
    "\n",
    "            if cfg.simcc:\n",
    "                pred_keypoints = np.expand_dims(out['keypoints'], axis=0)\n",
    "                pred_keypoint_scores = out['keypoint_scores']\n",
    "                keypoints_restored = np.dot(bb2img_trans, np.concatenate((pred_keypoints[0], np.ones((pred_keypoints[0].shape[0], 1))), axis=1).transpose(1, 0))\n",
    "                keypoints_restored = keypoints_restored[:2, :].transpose(1, 0)\n",
    "            else:\n",
    "                pred_keypoints = np.expand_dims(out['joints_coord_img'].cpu().numpy(), axis=0)\n",
    "                pred_keypoints[:,:,0] *= cfg.input_img_shape[1]\n",
    "                pred_keypoints[:,:,1] *= cfg.input_img_shape[0]\n",
    "                keypoints_restored = np.dot(bb2img_trans, np.concatenate((pred_keypoints[0], np.ones((pred_keypoints[0].shape[0], 1))), axis=1).transpose(1, 0))\n",
    "                keypoints_restored = keypoints_restored[:2, :].transpose(1, 0)\n",
    "                pred_keypoint_scores = np.any(keypoints_restored, axis=1)\n",
    "\n",
    "    #         # flip back to left hand\n",
    "    #         if annot['hand_type'] == 'left':\n",
    "    #             joints_out[:,0] *= -1 \n",
    "            _, avg_acc, _ = keypoint_pck_accuracy(\n",
    "                pred=np.expand_dims(keypoints_restored, axis=0),\n",
    "                gt=np.expand_dims(gt_joints_coord_img[:,:2], axis=0),\n",
    "                mask=np.expand_dims(pred_keypoint_scores, axis=0) > 0,\n",
    "                thr=cfg.pck_thr,\n",
    "                norm_factor=np.expand_dims(annot['img_shape'], axis=0),\n",
    "            )\n",
    "\n",
    "            self.eval_result[2].append(compute_mpjpe(keypoints_restored, gt_joints_coord_img[:,:2]))\n",
    "\n",
    "            if cfg.backbone == 'unext':\n",
    "                self.eval_result[0].append(num_correct / num_pixels)\n",
    "                self.eval_result[1].append(avg_acc)\n",
    "            else:\n",
    "                self.eval_result[0].append(avg_acc)\n",
    "                \n",
    "    def print_eval_result(self, test_epoch):\n",
    "        message = []\n",
    "        if cfg.backbone == 'unext':\n",
    "            message.append('Output: {0}, Model: snapshot_{1}.pth.tar'.format(cfg.output_dir.split('\\\\')[-1], test_epoch))\n",
    "            message.append('Correct/Total(One Batch) pixels: {0:.2f}'.format(np.mean(self.eval_result[0]) * 100))\n",
    "            message.append('PCK@{0}: {1:.2f}'.format(cfg.pck_thr, np.mean(self.eval_result[1]) * 100))\n",
    "            message.append('MPJPE : %.2f' % np.mean(self.eval_result[2]))\n",
    "        else:\n",
    "            message.append('Output: {0}, Model: snapshot_{1}.pth.tar'.format(cfg.output_dir.split('\\\\')[-1], test_epoch))\n",
    "            message.append('PCK@{0}: {1:.2f}'.format(cfg.pck_thr, np.mean(self.eval_result[0]) * 100))\n",
    "            message.append('MPJPE : %.2f' % np.mean(self.eval_result[2]))\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HO3D \n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "from common.logger import colorlogger\n",
    "from common.utils.skeleton_map import skeleton_map_gray\n",
    "from pycocotools.coco import COCO\n",
    "from main.config import cfg\n",
    "from common.utils.preprocessing import load_img, get_bbox, process_bbox, generate_patch_image, augmentation\n",
    "from common.utils.transforms import world2cam, cam2pixel, compute_mpjpe, compute_pa_mpjpe\n",
    "from common.utils.vis import vis_keypoints, vis_mesh, save_obj, vis_keypoints_with_skeleton\n",
    "from common.codecs.keypoint_eval import keypoint_pck_accuracy\n",
    "\n",
    "class HO3D(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, data_split, log_name='cfg_logs.txt'):\n",
    "        self.transform = transform\n",
    "        self.data_split = data_split if data_split == 'evaluation' else 'train'\n",
    "        self.root_dir = osp.join('data', 'HO3D')\n",
    "        self.annot_path = osp.join(self.root_dir, 'annotations')\n",
    "        self.root_joint_idx = 0\n",
    "        self.hand_type = {'left': 0, 'right': 0}\n",
    "        self.datalist = self.load_data()\n",
    "        if self.data_split == 'train':\n",
    "            self.eval_result = [[],[],[],[]]\n",
    "        \n",
    "        self.logger = colorlogger(cfg.log_dir, log_name=log_name)\n",
    "        \n",
    "        for i in cfg.__dict__:\n",
    "            self.logger.info('{0}: {1}'.format(i, cfg.__dict__[i]))\n",
    "        message = []\n",
    "        message.append(f\"DataList len: {len(self.datalist)}\")\n",
    "        message.append('left hand data: {0}, right hand data: {1}'.format(self.hand_type['left'], self.hand_type['right']))\n",
    "        \n",
    "        if cfg.simcc and cfg.SET:\n",
    "            message.append(f'Start the model {cfg.backbone} with SET and simcc')\n",
    "        elif cfg.simcc:\n",
    "            message.append(f'Start the model {cfg.backbone} without SET and with simcc')\n",
    "        elif cfg.SET:\n",
    "            message.append(f'Start the model {cfg.backbone} with SET and with regressor')\n",
    "        else:\n",
    "            message.append(f'Start the model {cfg.backbone} without SET and simcc')\n",
    "        for msg in message:\n",
    "            self.logger.info(msg)\n",
    "            \n",
    "    def load_data(self):\n",
    "        db = COCO(osp.join(self.annot_path, \"HO3D_{}_data.json\".format(self.data_split)))\n",
    "        # db = COCO(osp.join(self.annot_path, 'HO3Dv3_partial_test_multiseq_coco.json'))\n",
    "\n",
    "        datalist = []\n",
    "        skip = 1\n",
    "        if self.data_split == 'train':\n",
    "            skip_mode = cfg.train_skip\n",
    "            remainder = cfg.train_remainder\n",
    "        elif self.data_split == 'test':\n",
    "            skip_mode = cfg.test_skip\n",
    "            remainder = cfg.test_remainder\n",
    "\n",
    "        for aid in db.anns.keys():\n",
    "            if skip % skip_mode == remainder:\n",
    "                ann = db.anns[aid]\n",
    "                image_id = ann['image_id']\n",
    "                img = db.loadImgs(image_id)[0]\n",
    "                if osp.exists(osp.join(self.root_dir, self.data_split, img['file_name'])):\n",
    "                    img_path = osp.join(self.root_dir, self.data_split, img['file_name'])\n",
    "                    # TEMP\n",
    "                    # img_path = osp.join(self.root_dir, 'train', img['sequence_name'], 'rgb', img['file_name'])\n",
    "\n",
    "                    img_shape = (img['height'], img['width'])\n",
    "                    joints_coord_cam = np.array(ann['joints_coord_cam'], dtype=np.float32) # meter\n",
    "                    cam_param = {k:np.array(v, dtype=np.float32) for k,v in ann['cam_param'].items()}\n",
    "                    joints_coord_img = cam2pixel(joints_coord_cam, cam_param['focal'], cam_param['princpt'])\n",
    "                    bbox = get_bbox(joints_coord_img[:,:2], np.ones_like(joints_coord_img[:,0]), expansion_factor=1.5)\n",
    "                    bbox = process_bbox(bbox, img['width'], img['height'], expansion_factor=1.0)\n",
    "                    data = {\"img_path\": img_path, \"img_shape\": img_shape, \"joints_coord_img\": joints_coord_img,\n",
    "                            \"bbox\": bbox,}\n",
    "                        \n",
    "                    if all(val is not None for val in data.values()):\n",
    "                        datalist.append(data)\n",
    "                        self.hand_type['right'] += 1\n",
    "            skip += 1\n",
    "        return datalist\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = copy.deepcopy(self.datalist[idx])\n",
    "        img_path, img_shape, bbox = data['img_path'], data['img_shape'], data['bbox']\n",
    "\n",
    "        # img\n",
    "        img = load_img(img_path)\n",
    "        img, img2bb_trans, self.bb2img_trans, rot, scale = augmentation(img, bbox, self.data_split, do_flip=False)\n",
    "        img = self.transform(img.astype(np.float32))/255.\n",
    "\n",
    "        if self.data_split == 'train':\n",
    "            targets = {}\n",
    "            ## 2D joint coordinate\n",
    "            joints_img = data['joints_coord_img']\n",
    "            joints_img_xy1 = np.concatenate((joints_img[:,:2], np.ones_like(joints_img[:,:1])),1)\n",
    "            joints_img = np.dot(img2bb_trans, joints_img_xy1.transpose(1,0)).transpose(1,0)[:,:2]\n",
    "            if not cfg.simcc:\n",
    "                joints_img_copy = joints_img.copy()\n",
    "                ## normalize to [0,1]\n",
    "                joints_img_copy[:,0] /= cfg.input_img_shape[0]\n",
    "                joints_img_copy[:,1] /= cfg.input_img_shape[1]\n",
    "                targets['joints_img'] = joints_img_copy\n",
    "            else:\n",
    "                targets['joints_img'] = joints_img\n",
    "            \n",
    "            skeleton_map = skeleton_map_gray((cfg.input_img_shape[0], cfg.input_img_shape[1]), joints_img)\n",
    "            skeleton_map = self.transform(skeleton_map.astype(np.float32))/255.\n",
    "\n",
    "            inputs = {'img': img}\n",
    "            targets['skeleton_map'] = skeleton_map\n",
    "        else:\n",
    "            inputs = {'img': img}\n",
    "            targets = {}\n",
    "\n",
    "        return inputs, targets\n",
    "                  \n",
    "    def evaluate(self, outs, cur_sample_idx):\n",
    "        annots = self.datalist\n",
    "        sample_num = len(outs)\n",
    "        for n in range(sample_num):            \n",
    "            annot = annots[cur_sample_idx + n]\n",
    "            # cv2.namedWindow(annot['img_path'], 0)\n",
    "            \n",
    "            out = outs[n]\n",
    "            \n",
    "            # img convert\n",
    "            img = load_img(annot['img_path'])\n",
    "            orig_img = copy.deepcopy(img)\n",
    "            img, img2bb_trans, bb2img_trans, rot, scale = augmentation(img, annot['bbox'], self.data_split, do_flip=False)\n",
    "        \n",
    "    #         # GT and out['keypoints]\n",
    "            gt_joints_coord_img = annot['joints_coord_img']\n",
    "            joints_img_xy1 = np.concatenate((gt_joints_coord_img[:,:2], np.ones_like(gt_joints_coord_img[:,:1])),1)\n",
    "            joints_img = np.dot(img2bb_trans, joints_img_xy1.transpose(1,0)).transpose(1,0)[:,:2]\n",
    "            \n",
    "            if cfg.backbone == 'unext':\n",
    "                gt_skeleton_map = skeleton_map_gray((cfg.input_img_shape[0], cfg.input_img_shape[1]), joints_img)\n",
    "                gt_skeleton_map = gt_skeleton_map/255.\n",
    "                \n",
    "                pred_skeleton_map = (out['skeleton_map'].squeeze().cpu().numpy()).astype(float)# > 0.5\n",
    "                \n",
    "                ## show result\n",
    "                cat_imgs = ux_hon_result(orig_img, img, pred_skeleton_map, gt_skeleton_map)\n",
    "                cat_imgs = ux_hon_result_final(out, bb2img_trans, orig_img, img, cat_imgs)\n",
    "                \n",
    "                path = cfg.vis_dir+'/'+'_'.join(annot['img_path'].split('/'))\n",
    "                cv2.imwrite(path, cat_imgs)\n",
    "                # cv2.imshow(annot['img_path'], cat_imgs)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "\n",
    "                pred_skeleton_map = (out['skeleton_map'].squeeze().cpu().numpy()>0.5).astype(float)# > 0.5\n",
    " \n",
    "                num_correct = (pred_skeleton_map==gt_skeleton_map).sum()\n",
    "                num_pixels = cfg.input_img_shape[0] * cfg.input_img_shape[1]\n",
    "            else:\n",
    "                img_uint8 = cv2.resize(orig_img.astype(np.uint8), (cfg.input_img_shape[0], cfg.input_img_shape[1]))\n",
    "                rgb_img_uint8 = cv2.cvtColor(img_uint8.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                rgb_img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                ori_imgs = np.hstack([rgb_img_uint8, rgb_img])\n",
    "                cat_imgs = ux_hon_result_final(out, bb2img_trans, orig_img, img, ori_imgs)\n",
    "                \n",
    "                path = cfg.vis_dir+'/'+'_'.join(annot['img_path'].split('/'))\n",
    "                cv2.imwrite(path, cat_imgs)\n",
    "                # cv2.imshow(annot['img_path'], cat_imgs)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "\n",
    "            if cfg.simcc:\n",
    "                pred_keypoints = np.expand_dims(out['keypoints'], axis=0)\n",
    "                pred_keypoint_scores = out['keypoint_scores']\n",
    "                keypoints_restored = np.dot(bb2img_trans, np.concatenate((pred_keypoints[0], np.ones((pred_keypoints[0].shape[0], 1))), axis=1).transpose(1, 0))\n",
    "                keypoints_restored = keypoints_restored[:2, :].transpose(1, 0)\n",
    "            else:\n",
    "                pred_keypoints = np.expand_dims(out['joints_coord_img'].cpu().numpy(), axis=0)\n",
    "                pred_keypoints[:,:,0] *= cfg.input_img_shape[1]\n",
    "                pred_keypoints[:,:,1] *= cfg.input_img_shape[0]\n",
    "                keypoints_restored = np.dot(bb2img_trans, np.concatenate((pred_keypoints[0], np.ones((pred_keypoints[0].shape[0], 1))), axis=1).transpose(1, 0))\n",
    "                keypoints_restored = keypoints_restored[:2, :].transpose(1, 0)\n",
    "                pred_keypoint_scores = np.any(keypoints_restored, axis=1)\n",
    "\n",
    "    #         # flip back to left hand\n",
    "    #         if annot['hand_type'] == 'left':\n",
    "    #             joints_out[:,0] *= -1 \n",
    "            _, avg_acc, _ = keypoint_pck_accuracy(\n",
    "                pred=np.expand_dims(keypoints_restored, axis=0),\n",
    "                gt=np.expand_dims(gt_joints_coord_img[:,:2], axis=0),\n",
    "                mask=np.expand_dims(pred_keypoint_scores, axis=0) > 0,\n",
    "                thr=cfg.pck_thr,\n",
    "                norm_factor=np.expand_dims(annot['img_shape'], axis=0),\n",
    "            )\n",
    "\n",
    "            self.eval_result[2].append(compute_mpjpe(keypoints_restored, gt_joints_coord_img[:,:2]))\n",
    "\n",
    "            if cfg.backbone == 'unext':\n",
    "                self.eval_result[0].append(num_correct / num_pixels)\n",
    "                self.eval_result[1].append(avg_acc)\n",
    "            else:\n",
    "                self.eval_result[0].append(avg_acc)\n",
    "                \n",
    "    def print_eval_result(self, test_epoch):\n",
    "        message = []\n",
    "        if cfg.backbone == 'unext':\n",
    "            message.append('Output: {0}, Model: snapshot_{1}.pth.tar'.format(cfg.output_dir.split('\\\\')[-1], test_epoch))\n",
    "            message.append('Correct/Total(One Batch) pixels: {0:.2f}'.format(np.mean(self.eval_result[0]) * 100))\n",
    "            message.append('PCK@{0}: {1:.2f}'.format(cfg.pck_thr, np.mean(self.eval_result[1]) * 100))\n",
    "            message.append('MPJPE : %.2f' % np.mean(self.eval_result[2]))\n",
    "        else:\n",
    "            message.append('Output: {0}, Model: snapshot_{1}.pth.tar'.format(cfg.output_dir.split('\\\\')[-1], test_epoch))\n",
    "            message.append('PCK@{0}: {1:.2f}'.format(cfg.pck_thr, np.mean(self.eval_result[0]) * 100))\n",
    "            message.append('MPJPE : %.2f' % np.mean(self.eval_result[2]))\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base\n",
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import abc\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim\n",
    "import torchvision.transforms as transforms\n",
    "from common.timer import Timer\n",
    "from common.logger import colorlogger\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "from main.config import cfg\n",
    "# dynamic model import\n",
    "if cfg.backbone == 'fpn':\n",
    "    from main.model import get_model\n",
    "\n",
    "# dynamic dataset import\n",
    "# exec('from ' + cfg.trainset + ' import ' + cfg.trainset)\n",
    "# exec('from ' + cfg.testset + ' import ' + cfg.testset)\n",
    "\n",
    "class Base(object):\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    def __init__(self, log_name='logs.txt'):\n",
    "        \n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        # timer\n",
    "        self.tot_timer = Timer()\n",
    "        self.gpu_timer = Timer()\n",
    "        self.read_timer = Timer()\n",
    "\n",
    "        # logger\n",
    "        self.logger = colorlogger(cfg.log_dir, log_name=log_name)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _make_batch_generator(self):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _make_model(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HandOccNet Trainer\n",
    "class Trainer(Base):\n",
    "    def __init__(self):\n",
    "        super(Trainer, self).__init__(log_name = 'train_logs.txt')\n",
    "\n",
    "    def get_optimizer(self, model):\n",
    "        model_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        optimizer = torch.optim.Adam(model_params, lr=cfg.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def save_model(self, state, epoch):\n",
    "        file_path = osp.join(cfg.model_dir,'snapshot_{}.pth.tar'.format(str(epoch)))\n",
    "        torch.save(state, file_path)\n",
    "        self.logger.info(\"Write snapshot into {}\".format(file_path))\n",
    "\n",
    "    def load_model(self, model, optimizer):\n",
    "        model_file_list = glob.glob(osp.join(cfg.model_dir,'*.pth.tar'))\n",
    "        cur_epoch = max([int(file_name[file_name.find('snapshot_') + 9 : file_name.find('.pth.tar')]) for file_name in model_file_list])\n",
    "        ckpt_path = osp.join(cfg.model_dir, 'snapshot_' + str(cur_epoch) + '.pth.tar')\n",
    "        ckpt = torch.load(ckpt_path) \n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        #optimizer.load_state_dict(ckpt['optimizer'])\n",
    "\n",
    "        self.logger.info('Load checkpoint from {}'.format(ckpt_path))\n",
    "        return start_epoch, model, optimizer\n",
    "\n",
    "    def set_lr(self, epoch):\n",
    "        for e in cfg.lr_dec_epoch:\n",
    "            if epoch < e:\n",
    "                break\n",
    "        if epoch < cfg.lr_dec_epoch[-1]:\n",
    "            idx = cfg.lr_dec_epoch.index(e)\n",
    "            for g in self.optimizer.param_groups:\n",
    "                g['lr'] = cfg.lr * (cfg.lr_dec_factor ** idx)\n",
    "        else:\n",
    "            for g in self.optimizer.param_groups:\n",
    "                g['lr'] = cfg.lr * (cfg.lr_dec_factor ** len(cfg.lr_dec_epoch))\n",
    "\n",
    "    def get_lr(self):\n",
    "        for g in self.optimizer.param_groups:\n",
    "            cur_lr = g['lr']\n",
    "        return cur_lr\n",
    "    \n",
    "    def _make_batch_generator(self):\n",
    "        # data load and construct batch generator\n",
    "        self.logger.info(\"Creating dataset...\")\n",
    "        # Augment train data\n",
    "        # train_transforms = transforms.Compose([\n",
    "        #     transforms.Resize((256, 192)),\n",
    "        #     transforms.ToTensor()\n",
    "        # ])\n",
    "        train_dataset = eval(cfg.trainset)(transforms.ToTensor(), \"train\")\n",
    "            \n",
    "        self.itr_per_epoch = math.ceil(len(train_dataset) / cfg.num_gpus / cfg.train_batch_size)\n",
    "        self.batch_generator = DataLoader(dataset=train_dataset, batch_size=cfg.num_gpus*cfg.train_batch_size, shuffle=True, num_workers=cfg.num_thread, pin_memory=True)\n",
    "\n",
    "    def _make_model(self):\n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph and optimizer...\")\n",
    "        if cfg.SET:\n",
    "            self.logger.info(\"Creating model with SET...\")\n",
    "        else:\n",
    "            self.logger.info(\"Creating model without SET...\")\n",
    "        model = get_model('train')\n",
    "\n",
    "        model = DataParallel(model).cuda()\n",
    "        optimizer = self.get_optimizer(model)\n",
    "        if cfg.continue_train:\n",
    "            start_epoch, model, optimizer = self.load_model(model, optimizer)\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        model.train()\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HandOccNet Tester\n",
    "class Tester(Base):\n",
    "    def __init__(self):\n",
    "        super(Tester, self).__init__(log_name = 'test_logs.txt')\n",
    "\n",
    "    def _make_batch_generator(self):\n",
    "        # data load and construct batch generator\n",
    "        self.logger.info(\"Creating dataset...\")\n",
    "        # Augment train data\n",
    "        # test_transforms = transforms.Compose([\n",
    "        #     transforms.Resize((256, 192)),\n",
    "        #     transforms.ToTensor()\n",
    "        # ])\n",
    "        self.test_dataset = eval(cfg.testset)(transforms.ToTensor(), \"test\")\n",
    "        self.batch_generator = DataLoader(dataset=self.test_dataset, batch_size=cfg.num_gpus*cfg.test_batch_size, shuffle=False, num_workers=cfg.num_thread, pin_memory=True)\n",
    "       \n",
    "    def _make_model(self, test_epoch):\n",
    "        model_path = os.path.join(cfg.model_dir, 'snapshot_%d.pth.tar' % test_epoch)\n",
    "        assert os.path.exists(model_path), 'Cannot find model at ' + model_path\n",
    "        self.logger.info('Load checkpoint from {}'.format(model_path))\n",
    "        \n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph...\")\n",
    "        model = get_model('test')\n",
    "        model = DataParallel(model).cuda()\n",
    "        ckpt = torch.load(model_path)\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        model.eval()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def _evaluate(self, outs, cur_sample_idx):\n",
    "        eval_result = self.test_dataset.evaluate(outs, cur_sample_idx)\n",
    "        return eval_result\n",
    "\n",
    "    def _print_eval_result(self, test_epoch):\n",
    "        message = self.test_dataset.print_eval_result(test_epoch)\n",
    "        for msg in message:\n",
    "            self.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m07-12 17:26:43\u001b[0m Creating dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Using GPU: 0\n",
      "loading annotations into memory...\n",
      "Done (t=41.90s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m07-12 17:27:28\u001b[0m trainset: DEX_YCB\n",
      "\u001b[92m07-12 17:27:28\u001b[0m testset: DEX_YCB\n",
      "\u001b[92m07-12 17:27:28\u001b[0m train_skip: 1000\n",
      "\u001b[92m07-12 17:27:28\u001b[0m test_skip: 1000\n",
      "\u001b[92m07-12 17:27:28\u001b[0m train_remainder: 0\n",
      "\u001b[92m07-12 17:27:28\u001b[0m test_remainder: 0\n",
      "\u001b[92m07-12 17:27:28\u001b[0m input_img_shape: (256, 256)\n",
      "\u001b[92m07-12 17:27:28\u001b[0m backbone: fpn\n",
      "\u001b[92m07-12 17:27:28\u001b[0m skeleton_width: 5\n",
      "\u001b[92m07-12 17:27:28\u001b[0m SET: True\n",
      "\u001b[92m07-12 17:27:28\u001b[0m simcc: False\n",
      "\u001b[92m07-12 17:27:28\u001b[0m att: SG\n",
      "\u001b[92m07-12 17:27:28\u001b[0m mask: False\n",
      "\u001b[92m07-12 17:27:28\u001b[0m lr_dec_epoch: [10, 20, 30, 40, 50, 60]\n",
      "\u001b[92m07-12 17:27:28\u001b[0m end_epoch: 50\n",
      "\u001b[92m07-12 17:27:28\u001b[0m lr: 0.0001\n",
      "\u001b[92m07-12 17:27:28\u001b[0m lr_dec_factor: 0.9\n",
      "\u001b[92m07-12 17:27:28\u001b[0m ckpt_freq: 10\n",
      "\u001b[92m07-12 17:27:28\u001b[0m train_batch_size: 16\n",
      "\u001b[92m07-12 17:27:28\u001b[0m in_channels: 256\n",
      "\u001b[92m07-12 17:27:28\u001b[0m out_channels: 21\n",
      "\u001b[92m07-12 17:27:28\u001b[0m final_layer_kernel_size: 3\n",
      "\u001b[92m07-12 17:27:28\u001b[0m flatten_dims: 1024\n",
      "\u001b[92m07-12 17:27:28\u001b[0m gau_cfg: {'hidden_dims': 256, 's': 128, 'expansion_factor': 2, 'dropout_rate': 0.0, 'drop_path': 0.0, 'act_fn': 'ReLU', 'use_rel_bias': False, 'pos_enc': False}\n",
      "\u001b[92m07-12 17:27:28\u001b[0m codec: {'type': 'SimCCLabel', 'input_size': (256, 256), 'sigma': (5.66, 5.66), 'simcc_split_ratio': 2.0, 'normalize': False, 'use_dark': False}\n",
      "\u001b[92m07-12 17:27:28\u001b[0m W: 512\n",
      "\u001b[92m07-12 17:27:28\u001b[0m H: 512\n",
      "\u001b[92m07-12 17:27:28\u001b[0m loss: {'type': 'KLDiscretLoss', 'use_target_weight': True, 'beta': 10.0, 'label_softmax': True}\n",
      "\u001b[92m07-12 17:27:28\u001b[0m lambda_joints_img: 100\n",
      "\u001b[92m07-12 17:27:28\u001b[0m test_batch_size: 64\n",
      "\u001b[92m07-12 17:27:28\u001b[0m pck_thr: 0.05\n",
      "\u001b[92m07-12 17:27:28\u001b[0m num_thread: 0\n",
      "\u001b[92m07-12 17:27:28\u001b[0m gpu_ids: 0\n",
      "\u001b[92m07-12 17:27:28\u001b[0m num_gpus: 1\n",
      "\u001b[92m07-12 17:27:28\u001b[0m continue_train: False\n",
      "\u001b[92m07-12 17:27:28\u001b[0m outputname: output_fpn_SG_wSET_reg_0\n",
      "\u001b[92m07-12 17:27:28\u001b[0m cur_dir: c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\n",
      "\u001b[92m07-12 17:27:28\u001b[0m root_dir: c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\\..\n",
      "\u001b[92m07-12 17:27:28\u001b[0m data_dir: c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\\..\\data\n",
      "\u001b[92m07-12 17:27:28\u001b[0m output_dir: c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\\..\\output_fpn_SG_wSET_reg_0\n",
      "\u001b[92m07-12 17:27:28\u001b[0m model_dir: c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\\..\\output_fpn_SG_wSET_reg_0\\model_dump\n",
      "\u001b[92m07-12 17:27:28\u001b[0m vis_dir: c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\\..\\output_fpn_SG_wSET_reg_0\\vis\n",
      "\u001b[92m07-12 17:27:28\u001b[0m log_dir: c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\\..\\output_fpn_SG_wSET_reg_0\\log\n",
      "\u001b[92m07-12 17:27:28\u001b[0m result_dir: c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\\..\\output_fpn_SG_wSET_reg_0\\result\n",
      "\u001b[92m07-12 17:27:28\u001b[0m mano_path: c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\\..\\common\\utils\\manopth\n",
      "\u001b[92m07-12 17:27:28\u001b[0m DataList len: 40\n",
      "\u001b[92m07-12 17:27:28\u001b[0m left hand data: 21, right hand data: 19\n",
      "\u001b[92m07-12 17:27:28\u001b[0m Start the model fpn with SET and with regressor\n",
      "\u001b[92m07-12 17:27:28\u001b[0m Creating graph and optimizer...\n",
      "\u001b[92m07-12 17:27:28\u001b[0m Creating model with SET...\n",
      "c:\\Users\\kevin\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001b[92m07-12 17:27:30\u001b[0m Creating dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.20s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m07-12 17:27:39\u001b[0m DataList len: 9\n",
      "\u001b[92m07-12 17:27:39\u001b[0m DataList len: 9\n",
      "\u001b[92m07-12 17:27:39\u001b[0m left hand data: 4, right hand data: 5\n",
      "\u001b[92m07-12 17:27:39\u001b[0m left hand data: 4, right hand data: 5\n",
      "\u001b[92m07-12 17:27:39\u001b[0m Start the model fpn with SET and with regressor\n",
      "\u001b[92m07-12 17:27:39\u001b[0m Start the model fpn with SET and with regressor\n",
      "\u001b[92m07-12 17:28:24\u001b[0m Epoch 0/50 itr 0/3: lr: 0.0001 speed: 0.00(gpu0.00s r_data0.00s)s/itr 0.00h/epoch loss_joints_img: 12.1300\n",
      "\u001b[92m07-12 17:28:33\u001b[0m Epoch 0/50 itr 1/3: lr: 0.0001 speed: 0.00(gpu0.00s r_data0.00s)s/itr 0.00h/epoch loss_joints_img: 14.6716\n",
      "\u001b[92m07-12 17:28:38\u001b[0m Epoch 0/50 itr 2/3: lr: 0.0001 speed: 0.00(gpu0.00s r_data0.00s)s/itr 0.00h/epoch loss_joints_img: 9.0227\n",
      "\u001b[92m07-12 17:28:46\u001b[0m Epoch 1/50 itr 0/3: lr: 0.0001 speed: 0.00(gpu0.00s r_data0.00s)s/itr 0.00h/epoch loss_joints_img: 15.9373\n",
      "\u001b[92m07-12 17:28:54\u001b[0m Epoch 1/50 itr 1/3: lr: 0.0001 speed: 0.00(gpu0.00s r_data0.00s)s/itr 0.00h/epoch loss_joints_img: 7.8195\n",
      "\u001b[92m07-12 17:28:56\u001b[0m Epoch 1/50 itr 2/3: lr: 0.0001 speed: 0.00(gpu0.00s r_data0.00s)s/itr 0.00h/epoch loss_joints_img: 4.4930\n",
      "\u001b[92m07-12 17:29:03\u001b[0m Epoch 2/50 itr 0/3: lr: 0.0001 speed: 0.00(gpu0.00s r_data0.00s)s/itr 0.00h/epoch loss_joints_img: 5.9327\n",
      "\u001b[92m07-12 17:29:11\u001b[0m Epoch 2/50 itr 1/3: lr: 0.0001 speed: 0.00(gpu0.00s r_data0.00s)s/itr 0.00h/epoch loss_joints_img: 14.1609\n",
      "\u001b[92m07-12 17:29:12\u001b[0m Epoch 2/50 itr 2/3: lr: 0.0001 speed: 0.00(gpu0.00s r_data0.00s)s/itr 0.00h/epoch loss_joints_img: 5.0224\n",
      "\u001b[92m07-12 17:29:20\u001b[0m Epoch 3/50 itr 0/3: lr: 0.0001 speed: 0.00(gpu0.00s r_data0.00s)s/itr 0.00h/epoch loss_joints_img: 10.3325\n",
      "\u001b[92m07-12 17:29:28\u001b[0m Epoch 3/50 itr 1/3: lr: 0.0001 speed: 0.00(gpu0.11s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 8.2883\n",
      "\u001b[92m07-12 17:29:29\u001b[0m Epoch 3/50 itr 2/3: lr: 0.0001 speed: 7.57(gpu0.11s r_data0.17s)s/itr 0.01h/epoch loss_joints_img: 9.9970\n",
      "\u001b[92m07-12 17:29:37\u001b[0m Epoch 4/50 itr 0/3: lr: 0.0001 speed: 4.53(gpu0.11s r_data0.19s)s/itr 0.00h/epoch loss_joints_img: 6.6652\n",
      "\u001b[92m07-12 17:29:44\u001b[0m Epoch 4/50 itr 1/3: lr: 0.0001 speed: 5.56(gpu0.11s r_data0.20s)s/itr 0.00h/epoch loss_joints_img: 11.5057\n",
      "\u001b[92m07-12 17:29:46\u001b[0m Epoch 4/50 itr 2/3: lr: 0.0001 speed: 6.09(gpu0.11s r_data0.18s)s/itr 0.01h/epoch loss_joints_img: 16.9209\n",
      "\u001b[92m07-12 17:29:53\u001b[0m Epoch 5/50 itr 0/3: lr: 0.0001 speed: 5.17(gpu0.11s r_data0.20s)s/itr 0.00h/epoch loss_joints_img: 6.7997\n",
      "\u001b[92m07-12 17:30:01\u001b[0m Epoch 5/50 itr 1/3: lr: 0.0001 speed: 5.59(gpu0.12s r_data0.21s)s/itr 0.00h/epoch loss_joints_img: 12.9671\n",
      "\u001b[92m07-12 17:30:03\u001b[0m Epoch 5/50 itr 2/3: lr: 0.0001 speed: 5.89(gpu0.12s r_data0.20s)s/itr 0.00h/epoch loss_joints_img: 7.7669\n",
      "\u001b[92m07-12 17:30:11\u001b[0m Epoch 6/50 itr 0/3: lr: 0.0001 speed: 5.34(gpu0.12s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 8.9337\n",
      "\u001b[92m07-12 17:30:18\u001b[0m Epoch 6/50 itr 1/3: lr: 0.0001 speed: 5.62(gpu0.12s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 10.1705\n",
      "\u001b[92m07-12 17:30:20\u001b[0m Epoch 6/50 itr 2/3: lr: 0.0001 speed: 5.82(gpu0.12s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 3.1859\n",
      "\u001b[92m07-12 17:30:27\u001b[0m Epoch 7/50 itr 0/3: lr: 0.0001 speed: 5.43(gpu0.12s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 3.6694\n",
      "\u001b[92m07-12 17:30:35\u001b[0m Epoch 7/50 itr 1/3: lr: 0.0001 speed: 5.61(gpu0.12s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 14.5670\n",
      "\u001b[92m07-12 17:30:36\u001b[0m Epoch 7/50 itr 2/3: lr: 0.0001 speed: 5.77(gpu0.12s r_data0.21s)s/itr 0.00h/epoch loss_joints_img: 4.4115\n",
      "\u001b[92m07-12 17:30:44\u001b[0m Epoch 8/50 itr 0/3: lr: 0.0001 speed: 5.46(gpu0.12s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 7.4935\n",
      "\u001b[92m07-12 17:30:52\u001b[0m Epoch 8/50 itr 1/3: lr: 0.0001 speed: 5.61(gpu0.12s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 5.8626\n",
      "\u001b[92m07-12 17:30:53\u001b[0m Epoch 8/50 itr 2/3: lr: 0.0001 speed: 5.74(gpu0.12s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 4.2327\n",
      "\u001b[92m07-12 17:31:01\u001b[0m Epoch 9/50 itr 0/3: lr: 0.0001 speed: 5.49(gpu0.12s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 4.5378\n",
      "\u001b[92m07-12 17:31:09\u001b[0m Epoch 9/50 itr 1/3: lr: 0.0001 speed: 5.61(gpu0.12s r_data0.23s)s/itr 0.00h/epoch loss_joints_img: 7.6855\n",
      "\u001b[92m07-12 17:31:10\u001b[0m Epoch 9/50 itr 2/3: lr: 0.0001 speed: 5.73(gpu0.12s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 10.6838\n",
      "\u001b[92m07-12 17:31:11\u001b[0m Write snapshot into c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\\..\\output_fpn_SG_wSET_reg_0\\model_dump\\snapshot_10.pth.tar\n",
      "\u001b[92m07-12 17:31:11\u001b[0m Load checkpoint from c:\\Users\\kevin\\Desktop\\master_graduate_data\\handoccnet_exp\\main\\..\\output_fpn_SG_wSET_reg_0\\model_dump\\snapshot_10.pth.tar\n",
      "\u001b[92m07-12 17:31:11\u001b[0m Creating graph...\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n",
      "\u001b[92m07-12 17:31:15\u001b[0m Output: output_fpn_SG_wSET_reg_0, Model: snapshot_9.pth.tar\n",
      "\u001b[92m07-12 17:31:15\u001b[0m PCK@0.05: 20.11\n",
      "\u001b[92m07-12 17:31:15\u001b[0m MPJPE : 42.92\n",
      "\u001b[92m07-12 17:31:25\u001b[0m Epoch 10/50 itr 0/3: lr: 9e-05 speed: 5.52(gpu0.14s r_data0.23s)s/itr 0.00h/epoch loss_joints_img: 5.0388\n",
      "\u001b[92m07-12 17:31:33\u001b[0m Epoch 10/50 itr 1/3: lr: 9e-05 speed: 5.71(gpu0.14s r_data0.23s)s/itr 0.00h/epoch loss_joints_img: 3.0166\n",
      "\u001b[92m07-12 17:31:35\u001b[0m Epoch 10/50 itr 2/3: lr: 9e-05 speed: 5.81(gpu0.14s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 15.0154\n",
      "\u001b[92m07-12 17:31:43\u001b[0m Epoch 11/50 itr 0/3: lr: 9e-05 speed: 5.65(gpu0.14s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 6.2735\n",
      "\u001b[92m07-12 17:31:51\u001b[0m Epoch 11/50 itr 1/3: lr: 9e-05 speed: 5.75(gpu0.14s r_data0.23s)s/itr 0.00h/epoch loss_joints_img: 7.0724\n",
      "\u001b[92m07-12 17:31:53\u001b[0m Epoch 11/50 itr 2/3: lr: 9e-05 speed: 5.84(gpu0.14s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 3.8628\n",
      "\u001b[92m07-12 17:32:01\u001b[0m Epoch 12/50 itr 0/3: lr: 9e-05 speed: 5.70(gpu0.14s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 3.8026\n",
      "\u001b[92m07-12 17:32:09\u001b[0m Epoch 12/50 itr 1/3: lr: 9e-05 speed: 5.78(gpu0.14s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 9.8696\n",
      "\u001b[92m07-12 17:32:11\u001b[0m Epoch 12/50 itr 2/3: lr: 9e-05 speed: 5.86(gpu0.14s r_data0.22s)s/itr 0.00h/epoch loss_joints_img: 4.4427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m\n\u001b[0;32m     42\u001b[0m trainer\u001b[38;5;241m.\u001b[39mgpu_timer\u001b[38;5;241m.\u001b[39mtoc()\n\u001b[0;32m     43\u001b[0m screen \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m itr \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, cfg\u001b[38;5;241m.\u001b[39mend_epoch, itr, trainer\u001b[38;5;241m.\u001b[39mitr_per_epoch),\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr: \u001b[39m\u001b[38;5;132;01m%g\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (trainer\u001b[38;5;241m.\u001b[39mget_lr()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124mh/epoch\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (trainer\u001b[38;5;241m.\u001b[39mtot_timer\u001b[38;5;241m.\u001b[39maverage_time \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600.\u001b[39m \u001b[38;5;241m*\u001b[39m trainer\u001b[38;5;241m.\u001b[39mitr_per_epoch),\n\u001b[0;32m     49\u001b[0m     ]\n\u001b[1;32m---> 50\u001b[0m screen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m k, v\u001b[38;5;241m.\u001b[39mdetach()) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossatt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39msimcc:\n\u001b[0;32m     52\u001b[0m     screen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m k, v\u001b[38;5;241m.\u001b[39mdetach()) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m acc\u001b[38;5;241m.\u001b[39mitems()]\n",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m trainer\u001b[38;5;241m.\u001b[39mgpu_timer\u001b[38;5;241m.\u001b[39mtoc()\n\u001b[0;32m     43\u001b[0m screen \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m itr \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, cfg\u001b[38;5;241m.\u001b[39mend_epoch, itr, trainer\u001b[38;5;241m.\u001b[39mitr_per_epoch),\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr: \u001b[39m\u001b[38;5;132;01m%g\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (trainer\u001b[38;5;241m.\u001b[39mget_lr()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124mh/epoch\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (trainer\u001b[38;5;241m.\u001b[39mtot_timer\u001b[38;5;241m.\u001b[39maverage_time \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600.\u001b[39m \u001b[38;5;241m*\u001b[39m trainer\u001b[38;5;241m.\u001b[39mitr_per_epoch),\n\u001b[0;32m     49\u001b[0m     ]\n\u001b[1;32m---> 50\u001b[0m screen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m%.4f\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossatt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39msimcc:\n\u001b[0;32m     52\u001b[0m     screen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m k, v\u001b[38;5;241m.\u001b[39mdetach()) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m acc\u001b[38;5;241m.\u001b[39mitems()]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## HandOccNet train + test process\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from main.config import cfg\n",
    "\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer._make_batch_generator()\n",
    "trainer._make_model()\n",
    "\n",
    "tester = Tester()\n",
    "tester._make_batch_generator()\n",
    "\n",
    "# train\n",
    "for epoch in range(trainer.start_epoch, cfg.end_epoch):\n",
    "    \n",
    "    trainer.set_lr(epoch)\n",
    "    trainer.tot_timer.tic()\n",
    "    trainer.read_timer.tic()\n",
    "    for itr, (inputs, targets) in enumerate(trainer.batch_generator):\n",
    "        trainer.read_timer.toc()\n",
    "        trainer.gpu_timer.tic()\n",
    "\n",
    "        # forward\n",
    "        trainer.optimizer.zero_grad()\n",
    "        if cfg.simcc:\n",
    "            loss, acc = trainer.model(inputs, targets, 'train')\n",
    "        else:\n",
    "            loss = trainer.model(inputs, targets, 'train')\n",
    "\n",
    "        loss = {k:loss[k].mean() for k in loss}\n",
    "\n",
    "        # backward\n",
    "        sum(loss[k] for k in loss).backward()\n",
    "        trainer.optimizer.step()\n",
    "        trainer.gpu_timer.toc()\n",
    "        screen = [\n",
    "            'Epoch %d/%d itr %d/%d:' % (epoch, cfg.end_epoch, itr, trainer.itr_per_epoch),\n",
    "            'lr: %g' % (trainer.get_lr()),\n",
    "            'speed: %.2f(gpu%.2fs r_data%.2fs)s/itr' % (\n",
    "                trainer.tot_timer.average_time, trainer.gpu_timer.average_time, trainer.read_timer.average_time),\n",
    "            '%.2fh/epoch' % (trainer.tot_timer.average_time / 3600. * trainer.itr_per_epoch),\n",
    "            ]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in loss.items()]\n",
    "        if cfg.backbone == 'crossatt' or cfg.simcc:\n",
    "            screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in acc.items()]\n",
    "        trainer.logger.info(' '.join(screen))\n",
    "\n",
    "        trainer.tot_timer.toc()\n",
    "        trainer.tot_timer.tic()\n",
    "        trainer.read_timer.tic()\n",
    "    \n",
    "    if (epoch+1)%cfg.ckpt_freq== 0 or epoch+1 == cfg.end_epoch:\n",
    "        trainer.save_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.model.state_dict(),\n",
    "            'optimizer': trainer.optimizer.state_dict(),\n",
    "        }, epoch+1)\n",
    "\n",
    "        tester._make_model(epoch+1)\n",
    "\n",
    "        eval_result = {}\n",
    "        cur_sample_idx = 0\n",
    "        for itr, (inputs, targets) in enumerate(tqdm(tester.batch_generator)):\n",
    "            \n",
    "            # forward\n",
    "            with torch.no_grad():\n",
    "                out = tester.model(inputs, targets, 'test')\n",
    "            \n",
    "            # save output\n",
    "            out = {k: v for k,v in out.items()}\n",
    "            for k,v in out.items(): batch_size = out[k].shape[0]\n",
    "            out = [{k: v[bid] for k,v in out.items()} for bid in range(batch_size)]\n",
    "\n",
    "            # evaluate\n",
    "            tester._evaluate(out, cur_sample_idx)\n",
    "            cur_sample_idx += len(out)\n",
    "\n",
    "        tester._print_eval_result(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HandOccNet train process\n",
    "\n",
    "import argparse\n",
    "from main.config import cfg\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer._make_batch_generator()\n",
    "trainer._make_model()\n",
    "\n",
    "# train\n",
    "for epoch in range(trainer.start_epoch, cfg.end_epoch):\n",
    "    \n",
    "    trainer.set_lr(epoch)\n",
    "    trainer.tot_timer.tic()\n",
    "    trainer.read_timer.tic()\n",
    "    for itr, (inputs, targets) in enumerate(trainer.batch_generator):\n",
    "        trainer.read_timer.toc()\n",
    "        trainer.gpu_timer.tic()\n",
    "\n",
    "        # forward\n",
    "        trainer.optimizer.zero_grad()\n",
    "        if cfg.simcc:\n",
    "            loss, acc = trainer.model(inputs, targets, 'train')\n",
    "        else:\n",
    "            loss = trainer.model(inputs, targets, 'train')\n",
    "\n",
    "        loss = {k:loss[k].mean() for k in loss}\n",
    "\n",
    "        # backward\n",
    "        sum(loss[k] for k in loss).backward()\n",
    "        trainer.optimizer.step()\n",
    "        trainer.gpu_timer.toc()\n",
    "        screen = [\n",
    "            'Epoch %d/%d itr %d/%d:' % (epoch, cfg.end_epoch, itr, trainer.itr_per_epoch),\n",
    "            'lr: %g' % (trainer.get_lr()),\n",
    "            'speed: %.2f(gpu%.2fs r_data%.2fs)s/itr' % (\n",
    "                trainer.tot_timer.average_time, trainer.gpu_timer.average_time, trainer.read_timer.average_time),\n",
    "            '%.2fh/epoch' % (trainer.tot_timer.average_time / 3600. * trainer.itr_per_epoch),\n",
    "            ]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in loss.items()]\n",
    "        if cfg.simcc:\n",
    "            screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in acc.items()]\n",
    "        trainer.logger.info(' '.join(screen))\n",
    "\n",
    "        trainer.tot_timer.toc()\n",
    "        trainer.tot_timer.tic()\n",
    "        trainer.read_timer.tic()\n",
    "    \n",
    "    if (epoch+1)%cfg.ckpt_freq== 0 or epoch+1 == cfg.end_epoch:\n",
    "        trainer.save_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.model.state_dict(),\n",
    "            'optimizer': trainer.optimizer.state_dict(),\n",
    "        }, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HandOccNet test process\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from main.config import cfg\n",
    "\n",
    "test_epoch = [epoch for epoch in range(10, 71, 10)]\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "tester = Tester()\n",
    "tester._make_batch_generator()\n",
    "\n",
    "for epoch in test_epoch:\n",
    "    tester._make_model(epoch)\n",
    "\n",
    "    eval_result = {}\n",
    "    cur_sample_idx = 0\n",
    "    for itr, (inputs, targets) in enumerate(tqdm(tester.batch_generator)):\n",
    "        \n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            out = tester.model(inputs, targets, 'test')\n",
    "        \n",
    "        # save output\n",
    "        out = {k: v for k,v in out.items()}\n",
    "        for k,v in out.items(): batch_size = out[k].shape[0]\n",
    "        out = [{k: v[bid] for k,v in out.items()} for bid in range(batch_size)]\n",
    "\n",
    "        # evaluate\n",
    "        tester._evaluate(out, cur_sample_idx)\n",
    "        cur_sample_idx += len(out)\n",
    "\n",
    "    tester._print_eval_result(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UneXt + HandOccNet UH_Trainer\n",
    "from main.model_UX import get_UX_model\n",
    "from main.model_HON import get_HON_model\n",
    "\n",
    "class UH_Trainer(Base):\n",
    "    def __init__(self):\n",
    "        super(UH_Trainer, self).__init__(log_name = 'train_logs.txt')\n",
    "\n",
    "    def get_UX_optimizer(self, model):\n",
    "        # model_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.ux_lr)\n",
    "        return optimizer\n",
    "\n",
    "    def get_HON_optimizer(self, model):\n",
    "        # model_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.hon_lr)\n",
    "        return optimizer\n",
    "\n",
    "    def save_UX_model(self, state, epoch):\n",
    "        file_path = osp.join(cfg.model_dir,'snapshot_UX_{}.pth.tar'.format(str(epoch)))\n",
    "        torch.save(state, file_path)\n",
    "        self.logger.info(\"Write UX snapshot into {}\".format(file_path))\n",
    "\n",
    "    def save_HON_model(self, state, epoch):\n",
    "        file_path = osp.join(cfg.model_dir,'snapshot_HON_{}.pth.tar'.format(str(epoch)))\n",
    "        torch.save(state, file_path)\n",
    "        self.logger.info(\"Write HON snapshot into {}\".format(file_path))\n",
    "\n",
    "    def load_UX_model(self, model, optimizer):\n",
    "        model_file_list = glob.glob(osp.join(cfg.model_dir,'*.pth.tar'))\n",
    "        cur_epoch = max([int(file_name[file_name.find('snapshot_UX_') + 9 : file_name.find('.pth.tar')]) for file_name in model_file_list])\n",
    "        ckpt_path = osp.join(cfg.model_dir, 'snapshot_UX_' + str(cur_epoch) + '.pth.tar')\n",
    "        ckpt = torch.load(ckpt_path) \n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        #optimizer.load_state_dict(ckpt['optimizer'])\n",
    "\n",
    "        self.logger.info('Load checkpoint from {}'.format(ckpt_path))\n",
    "        return start_epoch, model, optimizer\n",
    "\n",
    "    def load_HON_model(self, model, optimizer):\n",
    "        model_file_list = glob.glob(osp.join(cfg.model_dir,'*.pth.tar'))\n",
    "        cur_epoch = max([int(file_name[file_name.find('snapshot_HON_') + 9 : file_name.find('.pth.tar')]) for file_name in model_file_list])\n",
    "        ckpt_path = osp.join(cfg.model_dir, 'snapshot_HON_' + str(cur_epoch) + '.pth.tar')\n",
    "        ckpt = torch.load(ckpt_path) \n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        #optimizer.load_state_dict(ckpt['optimizer'])\n",
    "\n",
    "        self.logger.info('Load checkpoint from {}'.format(ckpt_path))\n",
    "        return start_epoch, model, optimizer\n",
    "    \n",
    "    def set_ux_lr(self, epoch):\n",
    "        for e in cfg.lr_dec_epoch:\n",
    "            if epoch < e:\n",
    "                break\n",
    "        if epoch < cfg.lr_dec_epoch[-1]:\n",
    "            idx = cfg.lr_dec_epoch.index(e)\n",
    "            for g in self.ux_optimizer.param_groups:\n",
    "                g['lr'] = cfg.ux_lr * (cfg.lr_dec_factor ** idx)\n",
    "        else:\n",
    "            for g in self.ux_optimizer.param_groups:\n",
    "                g['lr'] = cfg.ux_lr * (cfg.lr_dec_factor ** len(cfg.lr_dec_epoch))\n",
    "\n",
    "    def set_hon_lr(self, epoch):\n",
    "        for e in cfg.lr_dec_epoch:\n",
    "            if epoch < e:\n",
    "                break\n",
    "        if epoch < cfg.lr_dec_epoch[-1]:\n",
    "            idx = cfg.lr_dec_epoch.index(e)\n",
    "            for g in self.hon_optimizer.param_groups:\n",
    "                g['lr'] = cfg.hon_lr * (cfg.lr_dec_factor ** idx)\n",
    "        else:\n",
    "            for g in self.hon_optimizer.param_groups:\n",
    "                g['lr'] = cfg.hon_lr * (cfg.lr_dec_factor ** len(cfg.lr_dec_epoch))\n",
    "\n",
    "    def get_ux_lr(self):\n",
    "        for g in self.ux_optimizer.param_groups:\n",
    "            cur_lr = g['lr']\n",
    "        return cur_lr\n",
    "    \n",
    "    def get_hon_lr(self):\n",
    "        for g in self.hon_optimizer.param_groups:\n",
    "            cur_lr = g['lr']\n",
    "        return cur_lr\n",
    "    \n",
    "    def _make_batch_generator(self):\n",
    "        # data load and construct batch generator\n",
    "        self.logger.info(\"Creating dataset...\")\n",
    "        # Augment train data\n",
    "        # train_transforms = transforms.Compose([\n",
    "        #     transforms.Resize((256, 192)),\n",
    "        #     transforms.ToTensor()\n",
    "        # ])\n",
    "        train_dataset = eval(cfg.trainset)(transforms.ToTensor(), \"train\")\n",
    "            \n",
    "        self.itr_per_epoch = math.ceil(len(train_dataset) / cfg.num_gpus / cfg.train_batch_size)\n",
    "        self.batch_generator = DataLoader(dataset=train_dataset, batch_size=cfg.num_gpus*cfg.train_batch_size, shuffle=True, num_workers=cfg.num_thread, pin_memory=True)\n",
    "\n",
    "    def _make_UX_model(self):\n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph and optimizer...\")\n",
    "        model = get_UX_model('train')\n",
    "\n",
    "        model = DataParallel(model).cuda()\n",
    "        ux_optimizer = self.get_UX_optimizer(model)\n",
    "        if cfg.continue_train:\n",
    "            start_epoch, model, ux_optimizer = self.load_model(model, ux_optimizer)\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        model.train()\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.ux_model = model\n",
    "        self.ux_optimizer = ux_optimizer\n",
    "\n",
    "    def _make_HON_model(self):\n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph and optimizer...\")\n",
    "        if cfg.SET:\n",
    "            self.logger.info(\"Creating model with SET...\")\n",
    "        else:\n",
    "            self.logger.info(\"Creating model without SET...\")\n",
    "        model = get_HON_model('train')\n",
    "\n",
    "        model = DataParallel(model).cuda()\n",
    "        hon_optimizer = self.get_HON_optimizer(model)\n",
    "        if cfg.continue_train:\n",
    "            start_epoch, model, hon_optimizer = self.load_model(model, hon_optimizer)\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        model.train()\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.hon_model = model\n",
    "        self.hon_optimizer = hon_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UneXt + HandOccNet UH_Tester\n",
    "from main.model_UX import get_UX_model\n",
    "from main.model_HON import get_HON_model\n",
    "\n",
    "class UH_Tester(Base):\n",
    "    def __init__(self):\n",
    "        super(UH_Tester, self).__init__(log_name = 'test_logs.txt')\n",
    "\n",
    "    def _make_batch_generator(self):\n",
    "        # data load and construct batch generator\n",
    "        self.logger.info(\"Creating dataset...\")\n",
    "        # Augment train data\n",
    "        # test_transforms = transforms.Compose([\n",
    "        #     transforms.Resize((256, 192)),\n",
    "        #     transforms.ToTensor()\n",
    "        # ])\n",
    "        self.test_dataset = eval(cfg.testset)(transforms.ToTensor(), \"test\")\n",
    "        self.batch_generator = DataLoader(dataset=self.test_dataset, batch_size=cfg.num_gpus*cfg.test_batch_size, shuffle=False, num_workers=cfg.num_thread, pin_memory=True)\n",
    "       \n",
    "    def _make_UX_model(self, test_epoch):\n",
    "        model_path = os.path.join(cfg.model_dir, 'snapshot_UX_%d.pth.tar' % test_epoch)\n",
    "        assert os.path.exists(model_path), 'Cannot find model at ' + model_path\n",
    "        self.logger.info('Load checkpoint from {}'.format(model_path))\n",
    "        \n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph...\")\n",
    "        model = get_UX_model('test')\n",
    "        model = DataParallel(model).cuda()\n",
    "        ckpt = torch.load(model_path)\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        model.eval()\n",
    "\n",
    "        self.ux_model = model\n",
    "\n",
    "    def _make_HON_model(self, test_epoch):\n",
    "        model_path = os.path.join(cfg.model_dir, 'snapshot_HON_%d.pth.tar' % test_epoch)\n",
    "        assert os.path.exists(model_path), 'Cannot find model at ' + model_path\n",
    "        self.logger.info('Load checkpoint from {}'.format(model_path))\n",
    "        \n",
    "        # prepare network\n",
    "        self.logger.info(\"Creating graph...\")\n",
    "        model = get_HON_model('test')\n",
    "        model = DataParallel(model).cuda()\n",
    "        ckpt = torch.load(model_path)\n",
    "        model.load_state_dict(ckpt['network'], strict=False)\n",
    "        model.eval()\n",
    "\n",
    "        self.hon_model = model\n",
    "\n",
    "    def _evaluate(self, outs, cur_sample_idx):\n",
    "        eval_result = self.test_dataset.evaluate(outs, cur_sample_idx)\n",
    "        return eval_result\n",
    "\n",
    "    def _print_eval_result(self, test_epoch):\n",
    "        message = self.test_dataset.print_eval_result(test_epoch)\n",
    "        for msg in message:\n",
    "            self.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unext + HandOccNet train + test process\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from main.config import cfg\n",
    "\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "trainer = UH_Trainer()\n",
    "trainer._make_batch_generator()\n",
    "trainer._make_UX_model()\n",
    "trainer._make_HON_model()\n",
    "\n",
    "tester = UH_Tester()\n",
    "tester._make_batch_generator()\n",
    "\n",
    "# train\n",
    "for epoch in range(trainer.start_epoch, cfg.end_epoch):\n",
    "    \n",
    "    trainer.set_ux_lr(epoch)\n",
    "    trainer.set_hon_lr(epoch)\n",
    "    trainer.tot_timer.tic()\n",
    "    trainer.read_timer.tic()\n",
    "    for itr, (inputs, targets) in enumerate(trainer.batch_generator):\n",
    "        trainer.read_timer.toc()\n",
    "        trainer.gpu_timer.tic()\n",
    "\n",
    "        ux_loss, ux_acc, ux_outs = trainer.ux_model(inputs, targets, 'train', itr)\n",
    "\n",
    "        ux_loss_dic = {k:ux_loss[k].mean() for k in ux_loss}\n",
    "        ux_loss = sum(ux_loss[k] for k in ux_loss_dic)\n",
    "\n",
    "        # ux forward\n",
    "        trainer.ux_optimizer.zero_grad()\n",
    "        # ux backward\n",
    "        ux_loss.backward(retain_graph=True)\n",
    "        trainer.ux_optimizer.step()\n",
    "\n",
    "        if cfg.simcc:\n",
    "            hon_loss, hon_acc = trainer.hon_model(inputs, targets, ux_outs, 'train')\n",
    "        else:\n",
    "            hon_loss = trainer.hon_model(inputs, targets, 'train')\n",
    "\n",
    "        hon_loss_dic = {k:hon_loss[k].mean() for k in hon_loss}\n",
    "        hon_loss = sum(hon_loss[k] for k in hon_loss_dic)\n",
    "\n",
    "        # hon forward\n",
    "        trainer.hon_optimizer.zero_grad()\n",
    "        # hon backward\n",
    "        hon_loss.backward()\n",
    "        trainer.hon_optimizer.step()\n",
    "\n",
    "        trainer.gpu_timer.toc()\n",
    "        screen = [\n",
    "            'Epoch %d/%d itr %d/%d:' % (epoch, cfg.end_epoch, itr, trainer.itr_per_epoch),\n",
    "            'ux_lr: %g' % (trainer.get_ux_lr()),\n",
    "            'hon_lr: %g' % (trainer.get_hon_lr()),\n",
    "            'speed: %.2f(gpu%.2fs r_data%.2fs)s/itr' % (\n",
    "                trainer.tot_timer.average_time, trainer.gpu_timer.average_time, trainer.read_timer.average_time),\n",
    "            '%.2fh/epoch' % (trainer.tot_timer.average_time / 3600. * trainer.itr_per_epoch),\n",
    "            ]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in ux_loss_dic.items()]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in hon_loss_dic.items()]\n",
    "        \n",
    "        screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in ux_acc.items()]\n",
    "        if cfg.simcc:\n",
    "            screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in hon_acc.items()]\n",
    "        trainer.logger.info(' '.join(screen))\n",
    "\n",
    "        trainer.tot_timer.toc()\n",
    "        trainer.tot_timer.tic()\n",
    "        trainer.read_timer.tic()\n",
    "    \n",
    "    if (epoch+1)%cfg.ckpt_freq== 0 or epoch+1 == cfg.end_epoch:\n",
    "        trainer.save_UX_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.ux_model.state_dict(),\n",
    "            'optimizer': trainer.ux_optimizer.state_dict(),\n",
    "        }, epoch+1)\n",
    "\n",
    "        trainer.save_HON_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.hon_model.state_dict(),\n",
    "            'optimizer': trainer.hon_optimizer.state_dict(),\n",
    "        }, epoch+1)\n",
    "\n",
    "        tester._make_UX_model(epoch+1)\n",
    "        tester._make_HON_model(epoch+1)\n",
    "\n",
    "        eval_result = {}\n",
    "        cur_sample_idx = 0\n",
    "        for itr, (inputs, targets) in enumerate(tqdm(tester.batch_generator)):\n",
    "            \n",
    "            # forward\n",
    "            with torch.no_grad():\n",
    "                ux_out = tester.ux_model(inputs, targets, 'test', itr)\n",
    "                hon_out = tester.hon_model(inputs, targets, ux_out, 'test')\n",
    "            \n",
    "            # save output\n",
    "            ux_out = {k: v for k,v in ux_out.items()}\n",
    "            for k,v in ux_out.items(): batch_size = ux_out[k].shape[0]\n",
    "            hon_out = {k: v for k,v in hon_out.items()}\n",
    "            for k,v in hon_out.items(): batch_size = hon_out[k].shape[0]\n",
    "            out = []\n",
    "            for bid in range(batch_size):\n",
    "                combined_dict = {}\n",
    "                for k,v in ux_out.items():\n",
    "                    combined_dict[k] = v[bid]\n",
    "                for k,v in hon_out.items():\n",
    "                    combined_dict[k] = v[bid]\n",
    "                out.append(combined_dict)\n",
    "                \n",
    "            # evaluate\n",
    "            tester._evaluate(out, cur_sample_idx)\n",
    "            cur_sample_idx += len(out)\n",
    "\n",
    "        tester._print_eval_result(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unext + HandOccNet train process\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from main.config import cfg\n",
    "\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "trainer = UH_Trainer()\n",
    "trainer._make_batch_generator()\n",
    "trainer._make_UX_model()\n",
    "trainer._make_HON_model()\n",
    "\n",
    "# train\n",
    "for epoch in range(trainer.start_epoch, cfg.end_epoch):\n",
    "    \n",
    "    trainer.set_ux_lr(epoch)\n",
    "    trainer.set_hon_lr(epoch)\n",
    "    trainer.tot_timer.tic()\n",
    "    trainer.read_timer.tic()\n",
    "    for itr, (inputs, targets) in enumerate(trainer.batch_generator):\n",
    "        trainer.read_timer.toc()\n",
    "        trainer.gpu_timer.tic()\n",
    "\n",
    "        ux_loss, ux_acc, ux_outs = trainer.ux_model(inputs, targets, 'train', itr)\n",
    "\n",
    "        ux_loss_dic = {k:ux_loss[k].mean() for k in ux_loss}\n",
    "        ux_loss = sum(ux_loss[k] for k in ux_loss_dic)\n",
    "\n",
    "        # ux forward\n",
    "        trainer.ux_optimizer.zero_grad()\n",
    "        # ux backward\n",
    "        ux_loss.backward(retain_graph=True)\n",
    "        trainer.ux_optimizer.step()\n",
    "\n",
    "        if cfg.simcc:\n",
    "            hon_loss, hon_acc = trainer.hon_model(inputs, targets, ux_outs, 'train')\n",
    "        else:\n",
    "            hon_loss = trainer.hon_model(inputs, targets, 'train')\n",
    "\n",
    "        hon_loss_dic = {k:hon_loss[k].mean() for k in hon_loss}\n",
    "        hon_loss = sum(hon_loss[k] for k in hon_loss_dic)\n",
    "\n",
    "        # hon forward\n",
    "        trainer.hon_optimizer.zero_grad()\n",
    "        # hon backward\n",
    "        hon_loss.backward()\n",
    "        trainer.hon_optimizer.step()\n",
    "\n",
    "        trainer.gpu_timer.toc()\n",
    "        screen = [\n",
    "            'Epoch %d/%d itr %d/%d:' % (epoch, cfg.end_epoch, itr, trainer.itr_per_epoch),\n",
    "            'ux_lr: %g' % (trainer.get_ux_lr()),\n",
    "            'hon_lr: %g' % (trainer.get_hon_lr()),\n",
    "            'speed: %.2f(gpu%.2fs r_data%.2fs)s/itr' % (\n",
    "                trainer.tot_timer.average_time, trainer.gpu_timer.average_time, trainer.read_timer.average_time),\n",
    "            '%.2fh/epoch' % (trainer.tot_timer.average_time / 3600. * trainer.itr_per_epoch),\n",
    "            ]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in ux_loss_dic.items()]\n",
    "        screen += ['%s: %.4f' % ('loss_' + k, v.detach()) for k,v in hon_loss_dic.items()]\n",
    "        \n",
    "        screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in ux_acc.items()]\n",
    "        if cfg.simcc:\n",
    "            screen += ['%s: %.4f' % ('acc_' + k, v.detach()) for k,v in hon_acc.items()]\n",
    "        trainer.logger.info(' '.join(screen))\n",
    "\n",
    "        trainer.tot_timer.toc()\n",
    "        trainer.tot_timer.tic()\n",
    "        trainer.read_timer.tic()\n",
    "    \n",
    "    if (epoch+1)%cfg.ckpt_freq== 0 or epoch+1 == cfg.end_epoch:\n",
    "        trainer.save_UX_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.ux_model.state_dict(),\n",
    "            'optimizer': trainer.ux_optimizer.state_dict(),\n",
    "        }, epoch+1)\n",
    "\n",
    "        trainer.save_HON_model({\n",
    "            'epoch': epoch,\n",
    "            'network': trainer.hon_model.state_dict(),\n",
    "            'optimizer': trainer.hon_optimizer.state_dict(),\n",
    "        }, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unext + HandOccNet test process\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from main.config import cfg\n",
    "\n",
    "cfg.set_args('0', False)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "tester = UH_Tester()\n",
    "tester._make_batch_generator()\n",
    "\n",
    "# train\n",
    "for epoch in range(9, cfg.end_epoch, 10):\n",
    "    tester._make_UX_model(epoch+1)\n",
    "    tester._make_HON_model(epoch+1)\n",
    "\n",
    "    eval_result = {}\n",
    "    cur_sample_idx = 0\n",
    "    for itr, (inputs, targets) in enumerate(tqdm(tester.batch_generator)):\n",
    "        \n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            ux_out = tester.ux_model(inputs, targets, 'test', itr)\n",
    "            hon_out = tester.hon_model(inputs, targets, ux_out, 'test')\n",
    "        \n",
    "        # save output\n",
    "        ux_out = {k: v for k,v in ux_out.items()}\n",
    "        for k,v in ux_out.items(): batch_size = ux_out[k].shape[0]\n",
    "        hon_out = {k: v for k,v in hon_out.items()}\n",
    "        for k,v in hon_out.items(): batch_size = hon_out[k].shape[0]\n",
    "        out = []\n",
    "        for bid in range(batch_size):\n",
    "            combined_dict = {}\n",
    "            for k,v in ux_out.items():\n",
    "                combined_dict[k] = v[bid]\n",
    "            for k,v in hon_out.items():\n",
    "                combined_dict[k] = v[bid]\n",
    "            out.append(combined_dict)\n",
    "            \n",
    "        # evaluate\n",
    "        tester._evaluate(out, cur_sample_idx)\n",
    "        cur_sample_idx += len(out)\n",
    "\n",
    "    tester._print_eval_result(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
